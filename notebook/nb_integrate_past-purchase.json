{
	"name": "nb_integrate_past-purchase",
	"properties": {
		"folder": {
			"name": "integration/past-purchase"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "synsppdlinte201",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f1838aa4-d731-4e06-8385-c31667a77650"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/68ab6d29-6524-4862-8fb0-8b171dcf03a9/resourceGroups/rg-synw-pdlintegrations-nonprd-dev-e2-01/providers/Microsoft.Synapse/workspaces/synw-pdlintegrations-nonprd-dev-e2-01/bigDataPools/synsppdlinte201",
				"name": "synsppdlinte201",
				"type": "Spark",
				"endpoint": "https://synw-pdlintegrations-nonprd-dev-e2-01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/synsppdlinte201",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**INITILIZATION OF VARIBLES**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"BASE_ADLS_CONN_STR = \"abfss://fs-syn-pdladf-dataint-nonprd-dev-e2-01@sasynpdladfdinpdeve201.dfs.core.windows.net/\"\r\n",
					"TARGET_STORE_ORDER_ITEM_PATH = \"opco/fdln/domain/order/store-order/master/order-item/\"\r\n",
					"TARGET_WEB_ORDER_ITEM_PATH=\"opco/fdln/domain/order/web-order/master/order-item/\"\r\n",
					"TARGET_USER_PATH = \"opco/fdln/domain/customer/user/master/\"\r\n",
					"TARGET_REPL_PROD_PATH =\"opco/fdln/domain/merch/product/retailer-cpt-it-rtlr-repl/master/product-id/\"\r\n",
					"TARGET_DEL_PROD_PATH =\"opco/fdln/domain/merch/product/retailer-cpt-past-purc-del/master/\"\r\n",
					"STAGE_STORE_ORDER_PATH= \"opco/fdln/domain/integration/past-purchase/stage/store-order/\"\r\n",
					"STAGE_WEB_ORDER_PATH=\"opco/fdln/domain/integration/past-purchase/stage/web-order\"\r\n",
					"STAGE_USER_PATH = \"opco/fdln/domain/integration/past-purchase/stage/user\"\r\n",
					"STAGE_REPL_PROD_PATH=\"opco/fdln/domain/integration/past-purchase/stage/replacement-products\"\r\n",
					"STAGE_DEL_PROD_PATH =\"opco/fdln/domain/integration/past-purchase/stage/deleted-products\"\r\n",
					"OUTGOING_CUSTOMER_PATH = \"opco/fdln/domain/customer/user/outgoing/2023-07-31-17-36-24/\"\r\n",
					"OUTGOING_STORE_ORDER_PATH = \"opco/fdln/domain/customer/user/outgoing/2023-07-31-17-36-24/\"\r\n",
					"OUTGOING_WEB_ORDER_PATH = \"opco/fdln/domain/customer/user/outgoing/2023-07-31-17-36-24/\"\r\n",
					"TARGET_PAST_PURC_PATH=\"opco/fdln/domain/integration/past-purchase/master\"\r\n",
					"NO_OF_PARTITIONS = 25\r\n",
					"YRS_DATA= 3\r\n",
					"IS_DELTA_LOAD = False\r\n",
					"OPCO = \"fdln\"\r\n",
					"USR_PAST_PURCHASE = \"past-purchase-integration-pipeline\"\r\n",
					"COSMOS_LINKED_SERVICE_NM = \"ls_az_cosmos_core_services_purchase_history_fdln\"\r\n",
					"COSMOS_CONTAINER_NM = \"past-purchase-test\"\r\n",
					"COSMOS_DB_NM = \"purchase_history\"\r\n",
					"\r\n",
					"is_stage_fl_exist= False\r\n",
					""
				],
				"execution_count": 63
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**INCLUDE UTILS**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/logging/nb_logging_util { LOGGER_NM: \"nb_integrate_past-purchase\", LOGGING_LEVEL: \"INFO\" }"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logger.info(\"***************************************************************************************\")\n",
					"logger.info(\"\\t\\t Starting Past Purchase Integration with below params - {0}\".format(OPCO))\n",
					"logger.info(\"***************************************************************************************\")\n",
					"logger.info(\"\\t\\t IS_DELTA_LOAD = {0}\".format(IS_DELTA_LOAD))\n",
					"logger.info(\"\\t\\t TARGET_USER_PATH = {0}\".format(TARGET_USER_PATH))\n",
					"logger.info(\"\\t\\t TARGET_WEB_ORDER_ITEM_PATH = {0}\".format(TARGET_WEB_ORDER_ITEM_PATH))\n",
					"logger.info(\"\\t\\t TARGET_STORE_ORDER_ITEM_PATH = {0}\".format(TARGET_STORE_ORDER_ITEM_PATH))\n",
					"logger.info(\"\\t\\t TARGET_DEL_PROD_PATH = {0}\".format(TARGET_DEL_PROD_PATH))\n",
					"logger.info(\"\\t\\t OUTGOING_CUSTOMER_PATH = {0}\".format(OUTGOING_CUSTOMER_PATH))\n",
					"logger.info(\"\\t\\t TARGET_PAST_PURC_PATH = {0}\".format(TARGET_PAST_PURC_PATH))\n",
					"logger.info(\"\\t\\t NO_OF_PARTITIONS = {0}\".format(NO_OF_PARTITIONS))\n",
					"logger.info(\"\\t\\t YRS_DATA = {0}\".format(YRS_DATA))\n",
					"logger.info(\"***************************************************************************************\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD STORE ORDER DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"from delta.tables import DeltaTable\r\n",
					"from pyspark.sql import functions as f\r\n",
					"\r\n",
					"def get_all_stor_ord():\r\n",
					"    curr_date= current_date()\r\n",
					"    from_date = curr_date - expr(\"INTERVAL \"+ str(YRS_DATA) +\" YEARS\")\r\n",
					"    df_store_order_mstr = spark \\\r\n",
					"        .read \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .load( BASE_ADLS_CONN_STR + TARGET_STORE_ORDER_ITEM_PATH) \r\n",
					"    df_store_order_mstr=df_store_order_mstr \\\r\n",
					"        .filter((df_store_order_mstr.transaction_ts >= from_date) & (df_store_order_mstr.transaction_ts <= curr_date)) \\\r\n",
					"        .filter(\"user_id IS NOT NULL\") \\\r\n",
					"        .withColumn(\"hash\", f.abs(f.hash(f.col(\"user_id\"))%NO_OF_PARTITIONS)) \\\r\n",
					"        .select(\"order_num\", \"loyalty_id\", \"transaction_ts\", \"user_id\", \"pod_id\", \"hash\")\r\n",
					"    logger.info(\"get_all_stor_ord : Extract ALL store orders between {0} and {1}. OPCO = {2}, Count = {3}\".format(from_date, curr_date, OPCO, df_store_order_mstr.count()))\r\n",
					"    return df_store_order_mstr\r\n",
					"\r\n",
					"def get_delta_stor_ord(df_mstr):\r\n",
					"    df_delta = spark \\\r\n",
					"        .read \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .option(\"inferSchema\", \"true\") \\\r\n",
					"        .load(BASE_ADLS_CONN_STR + OUTGOING_STORE_ORDER_PATH) \\\r\n",
					"        .select(\"user_id\") \\\r\n",
					"        .distinct()\r\n",
					"    df_delta_stor_ord = df_mstr \\\r\n",
					"        .join(df_delta, df_mstr[\"user_id\"] == df_delta[\"user_id\"], \"inner\") \\\r\n",
					"        .select(df_mstr[\"*\"])\r\n",
					"    logger.info(\"get_delta_stor_ord : Filter DELTA store orders. OPCO = {0}, Count = {1}\".format(OPCO, df_delta_stor_ord.count()))\r\n",
					"    return df_delta_stor_ord\r\n",
					"\r\n",
					"def write_stage_stor_ord(df_stg):\r\n",
					"    df_stg \\\r\n",
					"        .repartition(1, \"user_id\") \\\r\n",
					"        .sort(\"user_id\") \\\r\n",
					"        .write \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"hash\") \\\r\n",
					"        .save(BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH)\r\n",
					"    logger.info(\"write_stage_stor_ord : Write store orders into STG container. OPCO = {0}, Count = {1}, Path = {2}\".format(OPCO, df_stg.count(), STAGE_STORE_ORDER_PATH))\r\n",
					"\r\n",
					"#main starts here \r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH)):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH, recurse = True)\r\n",
					"    logger.info('Purging existing stage store order data')\r\n",
					"\r\n",
					"df_stor_ord_master = get_all_stor_ord()\r\n",
					"\r\n",
					"if IS_DELTA_LOAD:\r\n",
					"    df_stor_ord_tobe_processed = get_delta_stor_ord(df_stor_ord_master)\r\n",
					"    write_stage_stor_ord(df_stor_ord_tobe_processed)\r\n",
					"else:\r\n",
					"    write_stage_stor_ord(df_stor_ord_master)"
				],
				"execution_count": 64
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD WEB DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"from delta.tables import DeltaTable\r\n",
					"from pyspark.sql import functions as f\r\n",
					"\r\n",
					"def get_all_web_ord():\r\n",
					"    curr_date= current_date()\r\n",
					"    from_date = curr_date - expr(\"INTERVAL \"+ str(YRS_DATA) +\" YEARS\")\r\n",
					"    df_web_order_mstr = spark \\\r\n",
					"        .read \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .load(BASE_ADLS_CONN_STR + TARGET_WEB_ORDER_ITEM_PATH) \r\n",
					"    df_web_order_mstr=df_web_order_mstr.filter((df_web_order_mstr.orderDate >= from_date) & (df_web_order_mstr.orderDate<= curr_date) ) \\\r\n",
					"        .filter(\"userId IS NOT NULL\") \\\r\n",
					"        .withColumn(\"hash\", f.abs(f.hash(f.col(\"userId\"))%NO_OF_PARTITIONS)) \\\r\n",
					"        .withColumnRenamed(\"orderId\", \"order_id\") \\\r\n",
					"        .withColumnRenamed(\"orderDate\", \"order_date\") \\\r\n",
					"        .withColumnRenamed(\"orderTime\", \"order_time\") \\\r\n",
					"        .withColumnRenamed(\"deliveryDate\", \"delivery_date\") \\\r\n",
					"        .withColumnRenamed(\"userId\", \"user_id\") \\\r\n",
					"        .withColumnRenamed(\"podId\", \"pod_id\") \\\r\n",
					"        .select(\"order_id\", \"order_date\", \"delivery_date\", \"user_id\", \"pod_id\", \"hash\",\"order_time\")\r\n",
					"    logger.info(\"get_all_web_ord : Extract ALL web orders between {0} and {1}. OPCO = {2}, Count = {3}\".format(from_date, curr_date, OPCO, df_web_order_mstr.count()))\r\n",
					"    return df_web_order_mstr\r\n",
					"\r\n",
					"def get_delta_web_ord(df_mstr):\r\n",
					"    df_delta = spark \\\r\n",
					"        .read \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .option(\"inferSchema\", \"true\") \\\r\n",
					"        .load(BASE_ADLS_CONN_STR + OUTGOING_WEB_ORDER_PATH) \\\r\n",
					"        .select(\"user_id\") \\\r\n",
					"        .distinct()\r\n",
					"    df_delta_web_ord = df_mstr \\\r\n",
					"        .join(df_delta, df_mstr[\"user_id\"] == df_delta[\"user_id\"], \"inner\") \\\r\n",
					"        .select(df_mstr[\"*\"])\r\n",
					"    logger.info(\"get_delta_web_ord : Filter DELTA store orders. OPCO = {0}, Count = {1}\".format(OPCO, df_delta_web_ord.count()))\r\n",
					"    return df_delta_web_ord\r\n",
					"\r\n",
					"def write_stage_web_ord(df_stg):\r\n",
					"    df_stg \\\r\n",
					"        .repartition(1, \"user_id\") \\\r\n",
					"        .sort(\"user_id\") \\\r\n",
					"        .write \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"hash\") \\\r\n",
					"        .save(BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH)\r\n",
					"    logger.info(\"write_stage_web_ord : Write web orders into STG container. OPCO = {0}, Count = {1}, Path = {2}\".format(OPCO, df_stg.count(), STAGE_WEB_ORDER_PATH))\r\n",
					"\r\n",
					"# main starts here\r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH)):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH, recurse = True)\r\n",
					"    logger.info(\"Purging staging data\")\r\n",
					"\r\n",
					"df_web_ord_master = get_all_web_ord()\r\n",
					"\r\n",
					"if IS_DELTA_LOAD:\r\n",
					"    df_web_ord_tobe_processed = get_delta_web_ord(df_web_ord_master)\r\n",
					"    write_stage_web_ord(df_web_ord_tobe_processed)\r\n",
					"else:\r\n",
					"    write_stage_web_ord(df_web_ord_master)\r\n",
					""
				],
				"execution_count": 65
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD CUSTOMER DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import functions as f\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"def get_all_users():\r\n",
					"    df_usr_mstr = spark \\\r\n",
					"        .read \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .load(BASE_ADLS_CONN_STR + TARGET_USER_PATH) \\\r\n",
					"        .withColumn(\"hash\", f.abs(f.hash(f.col(\"user_id\"))%NO_OF_PARTITIONS))\r\n",
					"    logger.info(\"get_all_users : Extract ALL customers. OPCO = {0}, Count = {1}\".format(OPCO, df_usr_mstr.count()))\r\n",
					"    return df_usr_mstr\r\n",
					"\r\n",
					"def get_delta_users(df_mstr):\r\n",
					"    df_delta = spark \\\r\n",
					"    .read \\\r\n",
					"    .format(\"delta\") \\\r\n",
					"    .option(\"inferSchema\", \"true\") \\\r\n",
					"    .load(BASE_ADLS_CONN_STR + OUTGOING_CUSTOMER_PATH) \\\r\n",
					"    .withColumn(\"hash\", f.abs(f.hash(f.col(\"user_id\"))%NO_OF_PARTITIONS))\r\n",
					"    df_user_tobe_processed = df_mstr \\\r\n",
					"        .join(df_delta, (df_mstr.user_id == df_delta.user_id) & (df_mstr.hash == df_delta.hash), \"inner\") \\\r\n",
					"        .select(df_mstr[\"user_id\"],df_mstr[\"hndl_id\"], df_mstr[\"rtlr_card_id\"], df_mstr[\"mail_tx\"], df_mstr[\"stat_cd\"], df_mstr[\"opco_id\"], df_mstr[\"hash\"])\r\n",
					"    logger.info(\"get_delta_users : Filter DELTA customers. OPCO = {0}, Count = {1}\".format(OPCO, df_user_tobe_processed.count()))\r\n",
					"    return df_user_tobe_processed\r\n",
					"\r\n",
					"def write_stage_user(df_stg):\r\n",
					"    df_stg \\\r\n",
					"        .repartition(1, \"user_id\") \\\r\n",
					"        .sort(\"user_id\") \\\r\n",
					"        .write \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"hash\") \\\r\n",
					"        .save(BASE_ADLS_CONN_STR + STAGE_USER_PATH)\r\n",
					"    logger.info(\"write_stage_user : Write customers into STG container. OPCO = {0}, Count = {1}, Path = {2}\".format(OPCO, df_stg.count(), STAGE_USER_PATH))\r\n",
					"\r\n",
					"#main starts here\r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_USER_PATH)):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_USER_PATH, recurse = True)\r\n",
					"    logger.info('Purging customer staging data')\r\n",
					"\r\n",
					"df_users_master = get_all_users()\r\n",
					"\r\n",
					"if IS_DELTA_LOAD:\r\n",
					"    df_users_tobe_processed = get_delta_users(df_users_master)\r\n",
					"    write_stage_user(df_users_tobe_processed)\r\n",
					"else:\r\n",
					"    write_stage_user(df_users_master)"
				],
				"execution_count": 66
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD REPLACEMENT PRODUCTS DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH)):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH, recurse = True)\r\n",
					"    logger.info(\"replcamen-product: Purging repalcement products from STG container. OPCO={0}\".format(OPCO))\r\n",
					"\r\n",
					"df_repl_prod = spark \\\r\n",
					"    .read \\\r\n",
					"    .format(\"delta\") \\\r\n",
					"    .load(BASE_ADLS_CONN_STR + TARGET_REPL_PROD_PATH) \r\n",
					"\r\n",
					"df_repl_prod=df_repl_prod.filter(\"prod_id IS NOT NULL\") \\\r\n",
					"    .filter(size(df_repl_prod.replacement_prod_ids) >0)\r\n",
					"                            \r\n",
					"df_repl_prod\\\r\n",
					"        .repartition(1) \\\r\n",
					"        .write \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"ecom_stor_id\") \\\r\n",
					"        .save(BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH)\r\n",
					"logger.info(\"replcamen-product: Write repalcement-products into STG container. OPCO = {0}, Count = {1}\".format(OPCO, df_repl_prod.count()))"
				],
				"execution_count": 67
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD DELETED PRODUCTS DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import functions as f\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH)):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH, recurse = True)\r\n",
					"    logger.info(\"deleted-product: Purging deleted products from STG container. OPCO = {0}\".format(OPCO))\r\n",
					"\r\n",
					"df_del_prod = spark \\\r\n",
					"    .read \\\r\n",
					"    .format(\"delta\") \\\r\n",
					"    .load(BASE_ADLS_CONN_STR + TARGET_DEL_PROD_PATH) \\\r\n",
					"    .withColumn(\"hash\", f.abs(f.hash(f.col(\"user_id\"))%NO_OF_PARTITIONS))\r\n",
					"                            \r\n",
					"df_del_prod\\\r\n",
					"    .repartition(1, \"user_id\") \\\r\n",
					"    .sort(\"user_id\") \\\r\n",
					"    .write \\\r\n",
					"    .format(\"delta\") \\\r\n",
					"    .mode(\"overwrite\") \\\r\n",
					"    .partitionBy(\"hash\") \\\r\n",
					"    .save(BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH)\r\n",
					"logger.info(\"deleted-product: Write deleted-products into STG container. OPCO = {0}, Count = {1}\".format(OPCO, df_del_prod.count()))"
				],
				"execution_count": 68
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**LOAD DATA FROM STAGE**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from delta.tables import DeltaTable\r\n",
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql import functions as f\r\n",
					"import datetime\r\n",
					"from pyspark.sql.types import StringType, ArrayType, DecimalType, IntegerType\r\n",
					"\r\n",
					"def check_if_stage_file_exists():\r\n",
					"    return DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH) & \\\r\n",
					"           DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH) & \\\r\n",
					"           DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_USER_PATH) & \\\r\n",
					"           DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH) & \\\r\n",
					"           DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH)\r\n",
					"\r\n",
					"\r\n",
					"is_stage_fl_exist = check_if_stage_file_exists()\r\n",
					"if is_stage_fl_exist == False:  \r\n",
					"    logger.error(\"check_if_stage_file_exists: Some of the input files don't exist in STG container. Check the below input paths \\n. {0}\\n{1}\\n{2}\\n{3}\\n{4}\".format(\r\n",
					"        STAGE_STORE_ORDER_PATH,\r\n",
					"        STAGE_WEB_ORDER_PATH,\r\n",
					"        STAGE_USER_PATH,\r\n",
					"        STAGE_REPL_PROD_PATH,\r\n",
					"        STAGE_DEL_PROD_PATH\r\n",
					"    ))\r\n",
					"    raise Exception(\"check_if_stage_file_exists: Some of the input files don't exist in STG container.\")\r\n",
					"\r\n",
					"#tbl_store_order\r\n",
					"df_store_order = spark.read \\\r\n",
					"                        .format(\"delta\") \\\r\n",
					"                        .load(BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH) \\\r\n",
					"                        .select(\"transaction_ts\", \"user_id\", \"pod_id\",\"loyalty_id\")\r\n",
					"df_store_order.createOrReplaceTempView(\"tbl_store_order\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_store_order] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_store_order.count()))\r\n",
					"\r\n",
					"#tbl_web_order\r\n",
					"df_web_order = spark \\\r\n",
					"                    .read \\\r\n",
					"                    .format(\"delta\") \\\r\n",
					"                    .load(BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH) \\\r\n",
					"                    .withColumn(\"order_date_time\",to_timestamp(concat(to_date(col(\"order_date\"),\"yyyy-MM-dd\").cast(\"string\"), lit(\" \"), col(\"order_time\")), \"yyyy-MM-dd HH:mm:ss\"))\\\r\n",
					"                    .withColumn(\"delivery_date_time\",to_timestamp(concat(to_date(col(\"delivery_date\"),\"yyyy-MM-dd\").cast(\"string\"), lit(\" \"), lit(\"00:00:00\")), \"yyyy-MM-dd HH:mm:ss\"))\\\r\n",
					"                    .select(\"order_time\",\"order_date\", \"delivery_date\", \"user_id\", \"pod_id\",  \"order_date_time\", \"delivery_date_time\")\r\n",
					"df_web_order.createOrReplaceTempView(\"tbl_web_order\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_web_order] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_web_order.count()))\r\n",
					"\r\n",
					"#tbl_repl_prod_id\r\n",
					"df_repl_prod_id = spark \\\r\n",
					"                    .read \\\r\n",
					"                    .format(\"delta\") \\\r\n",
					"                    .load(BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH)\r\n",
					"df_repl_prod_id.createOrReplaceTempView(\"tbl_repl_prod_id\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_repl_prod_id] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_repl_prod_id.count()))\r\n",
					"\r\n",
					"#tbl_users\r\n",
					"df_users = spark \\\r\n",
					"        .read \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .load(BASE_ADLS_CONN_STR + STAGE_USER_PATH) \\\r\n",
					"        .select(\"user_id\", \"hndl_id\", \"rtlr_card_id\", \"mail_tx\", \"stat_cd\", \"opco_id\")\r\n",
					"df_users.createOrReplaceTempView(\"tbl_users\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_users] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_users.count()))\r\n",
					"\r\n",
					"#tbl_deleted_prod\r\n",
					"df_deleted_prod= spark \\\r\n",
					"                .read \\\r\n",
					"                .format(\"delta\") \\\r\n",
					"                .load(BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH)\r\n",
					"df_deleted_prod.createOrReplaceTempView(\"tbl_deleted_prod\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_deleted_prod] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_deleted_prod.count()))\r\n",
					"\r\n",
					"logger.info(\"stage-data-load: Created required all stage tables for all domains. Ready to start the integration. OPCO={0}\".format(OPCO))"
				],
				"execution_count": 69
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**JOIN STORE WEB DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_union_query = spark.sql(\r\n",
					"\r\n",
					"\r\n",
					"                    \"\"\"\r\n",
					"                    SELECT transaction_ts as order_date,user_id,pod_id\r\n",
					"                    FROM tbl_store_order\r\n",
					"                    UNION ALL\r\n",
					"                    SELECT delivery_date_time as order_date,user_id,pod_id\r\n",
					"                    FROM tbl_web_order\r\n",
					"                    \"\"\"\r\n",
					"\r\n",
					"                    )\r\n",
					"\r\n",
					"df_union_query.createOrReplaceTempView(\"tbl_web_store_order\")\r\n",
					"logger.info(\"merge-web-store-orders: Merged store and web orders into a single dataset. OPCO = {0}\".format(OPCO)) "
				],
				"execution_count": 70
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**SPLITTING WEB STORE DATA INTO 3 MONTHS, 6 MONTHS & ALL TIMES**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"logger.info(\"aggregate-products: Aggregate 3Months, 6Months and AllTime Products. OPCO = {0}\".format(OPCO)) \r\n",
					"df_query = spark.sql(\"\"\"\r\n",
					"                        SELECT  a.user_id,\r\n",
					"                        a.pod_id,\r\n",
					"                        array_distinct(concat(array(a.pod_id), coalesce(b.replacement_prod_ids,array()))) repl_prod_ids_lst,                     \r\n",
					"                        CASE WHEN months_between(current_date(),a.order_date)<=3 THEN TRUE ELSE FALSE END as orderWindowThreeMonths,\r\n",
					"                        CASE WHEN months_between(current_date(),a.order_date)<=6 THEN TRUE ELSE FALSE END as orderWindowSixMonths,\r\n",
					"                        CASE WHEN months_between(current_date(),a.order_date)<=36 THEN TRUE ELSE FALSE END as orderWindowAllTimes\r\n",
					"                        FROM tbl_web_store_order a\r\n",
					"                        LEFT JOIN tbl_repl_prod_id b ON a.pod_id=b.prod_id\r\n",
					"\r\n",
					"                    \"\"\")\r\n",
					"\r\n",
					"df_usr_prd_three_months = df_query \\\r\n",
					"                                .where(f.col(\"orderWindowThreeMonths\") == True) \\\r\n",
					"                                .groupBy(\"user_id\") \\\r\n",
					"                                .agg(f.flatten(f.collect_set(\"repl_prod_ids_lst\")).alias(\"three_months\")) \\\r\n",
					"                                .repartition(100, \"user_id\")\r\n",
					"\r\n",
					"df_usr_prd_three_months =df_usr_prd_three_months.withColumnRenamed(\"user_id\",\"user_id_three_months\")\r\n",
					"\r\n",
					"\r\n",
					"df_usr_prd_six_months = df_query \\\r\n",
					"                                .where(f.col(\"orderWindowSixMonths\") == True) \\\r\n",
					"                                .groupBy(\"user_id\") \\\r\n",
					"                                .agg(f.flatten(f.collect_set(\"repl_prod_ids_lst\")).alias(\"six_months\"))  \\\r\n",
					"                                .repartition(100, \"user_id\")\r\n",
					"\r\n",
					"df_usr_prd_six_months =df_usr_prd_six_months.withColumnRenamed(\"user_id\",\"user_id_six_months\")   \r\n",
					"                                          \r\n",
					"\r\n",
					"df_usr_prd_all_times = df_query \\\r\n",
					"                                .where(f.col(\"orderWindowAllTimes\") == True) \\\r\n",
					"                                .groupBy(\"user_id\") \\\r\n",
					"                                .agg(f.flatten(f.collect_set(\"repl_prod_ids_lst\")).alias(\"all_times\"))  \\\r\n",
					"                                .repartition(100, \"user_id\")\r\n",
					"\r\n",
					"df_usr_prd_all_times=df_usr_prd_all_times.withColumnRenamed(\"user_id\",\"user_id_all\")\r\n",
					""
				],
				"execution_count": 71
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**LAST ORDER DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"logger.info(\"aggregate-products: Aggregate LatestOrder Products. OPCO = {0}\".format(OPCO)) \r\n",
					"\r\n",
					"df_last_order_query = spark.sql(\"\"\"\r\n",
					"                        SELECT\r\n",
					"\t\t                a.user_id,\t\t                \r\n",
					"\t\t                a.pod_id,\r\n",
					"                        array_distinct(concat(array(a.pod_id), coalesce(c.replacement_prod_ids,array()))) pod_id_lst\r\n",
					"                    FROM tbl_web_store_order a\r\n",
					"                        INNER JOIN\r\n",
					"                        (\r\n",
					"                            SELECT user_id,MAX(order_date) last_ordered_time\r\n",
					"                            FROM tbl_web_store_order\r\n",
					"                            GROUP BY user_id\r\n",
					"                        ) b ON a.user_id=b.user_id AND a.order_date=b.last_ordered_time\r\n",
					"                        LEFT JOIN tbl_repl_prod_id c ON a.pod_id=c.prod_id\r\n",
					"                    \"\"\")\r\n",
					"\r\n",
					"df_usr_prd_last_order = df_last_order_query \\\r\n",
					"                                .groupBy(\"user_id\") \\\r\n",
					"                                .agg(f.flatten(f.collect_set(\"pod_id_lst\")).alias(\"last_order\"))  \\\r\n",
					"                                .repartition(100, \"user_id\")\r\n",
					"\r\n",
					"df_usr_prd_last_order = df_usr_prd_last_order.withColumnRenamed(\"user_id\",\"user_id_last_order\") \r\n",
					""
				],
				"execution_count": 72
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**REMOVED PURCHASES DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logger.info(\"aggregate-products: Remove user deleted past-purchase products. OPCO = {0}\".format(OPCO)) \r\n",
					"\r\n",
					"df_removed_purc=spark.sql(\r\n",
					"                        \"\"\"\r\n",
					"                        SELECT DISTINCT\r\n",
					"                        a.user_id as user_id_del,\r\n",
					"                        collect_set(a.pod_id) OVER(PARTITION BY a.user_id) as removed_purchases\r\n",
					"                        FROM tbl_web_store_order a\r\n",
					"                        INNER JOIN\r\n",
					"                        tbl_deleted_prod b ON a.user_id=b.user_id AND a.pod_id=b.prod_id AND a.order_date> b.audt_cr_dt_tm\r\n",
					"                        \"\"\")"
				],
				"execution_count": 73
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**PAST PURCHASE MANAGEMENT**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.types import StringType, ArrayType, NullType\r\n",
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"logger.info(\"aggregate-products: Aggregate customer and their past-purchases. OPCO = {0}\".format(OPCO)) \r\n",
					"\r\n",
					"df_past_purc = df_users.join(df_usr_prd_all_times, df_users.user_id == df_usr_prd_all_times.user_id_all, \"left\") \\\r\n",
					"                        .join(df_usr_prd_six_months, df_users.user_id == df_usr_prd_six_months.user_id_six_months, \"left\") \\\r\n",
					"                        .join(df_usr_prd_three_months, df_users.user_id == df_usr_prd_three_months.user_id_three_months, \"left\") \\\r\n",
					"                        .join(df_usr_prd_last_order, df_users.user_id == df_usr_prd_last_order.user_id_last_order, \"left\") \\\r\n",
					"                        .join(df_removed_purc, df_users.user_id == df_removed_purc.user_id_del, \"left\") \\\r\n",
					"                        .select(\"user_id\",\"hndl_id\", \"rtlr_card_id\", \"mail_tx\", \"stat_cd\", \"opco_id\", \\\r\n",
					"                        coalesce(\"three_months\",array()).alias(\"three_months\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"six_months\",array()).alias(\"six_months\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"all_times\",array()).alias(\"all_times\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"last_order\",array()).alias(\"last_order\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"removed_purchases\",array()).alias(\"removed_purchases\").cast(ArrayType(IntegerType()))).distinct() \r\n",
					"\r\n",
					"df_past_purc = df_past_purc \\\r\n",
					"        .withColumn(\"id\", F.col(\"user_id\").cast(\"string\")) \\\r\n",
					"        .withColumn(\"ordergenius_rank_partition\", F.lit(None).cast(NullType())) \\\r\n",
					"        .withColumn(\"ordergenius_default\",array([]).cast(ArrayType(IntegerType()))) \\\r\n",
					"        .withColumn(\"ordergenius_set_recency_decay\",array([]).cast(ArrayType(IntegerType()))) \\\r\n",
					"        .withColumn(\"first_lpurc_dt\", F.lit(None).cast(NullType())) \\\r\n",
					"        .withColumn(\"last_lpurc_dt\", F.lit(None).cast(NullType())) \\\r\n",
					"        .withColumn(\"last_purc_dates\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"avg_qtys\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"qtys\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"categories\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"category_averages\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"total_orders\",lit(0).cast(IntegerType())) \\\r\n",
					"        .withColumn(\"total_amount\",lit(0.00).cast(DecimalType(10,2))) \\\r\n",
					"        .withColumn(\"avg_ord_amt\",lit(0.00).cast(DecimalType(10,2))) \\\r\n",
					"        .withColumn(\"max_tot_purc_qy\",lit(0).cast(IntegerType())) \\\r\n",
					"        .withColumn(\"last_updated_at\", current_timestamp()) \\\r\n",
					"        .withColumn(\"last_updated_by\",lit(USR_PAST_PURCHASE))\r\n",
					"                           \r\n",
					"\r\n",
					"df_past_purc =  df_past_purc.withColumn(\"hash\", f.abs(f.hash(f.col(\"user_id\"))%NO_OF_PARTITIONS))\r\n",
					"\r\n",
					"df_past_purc \\\r\n",
					"        .repartition(1, \"user_id\") \\\r\n",
					"        .sort(\"user_id\") \\\r\n",
					"        .write \\\r\n",
					"        .option(\"mergeSchema\", \"true\") \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"hash\") \\\r\n",
					"        .save(BASE_ADLS_CONN_STR + TARGET_PAST_PURC_PATH)\r\n",
					"\r\n",
					"logger.info(\"persist: Customers' past-purchase has been processed and persisted for OPCO = {0}, Count={1}, Path={2}\".format(OPCO, df_past_purc.count(), TARGET_PAST_PURC_PATH)) \r\n",
					""
				],
				"execution_count": 74
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**LOAD PAST PURCHASE INTO COSMOS DATASTORE**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import functions as F \n",
					"\n",
					"logger.info(\"persist: Started persisting past-purchase in Cosmos for OPCO = {0}, Count={1}, Container={2}\".format(OPCO, df_past_purc.count(), COSMOS_CONTAINER_NM)) \n",
					"\n",
					"cfg = {\n",
					"    \"spark.synapse.linkedService\": COSMOS_LINKED_SERVICE_NM,\n",
					"    \"spark.cosmos.container\": COSMOS_CONTAINER_NM\n",
					"}\n",
					"\n",
					"\n",
					"df_past_purc_data_store = df_past_purc \\\n",
					"    .withColumnRenamed(\"user_id\", \"userId\") \\\n",
					"    .withColumnRenamed(\"hndl_id\", \"hndlId\") \\\n",
					"    .withColumnRenamed(\"rtlr_card_id\", \"rtlrCardId\") \\\n",
					"    .withColumnRenamed(\"mail_tx\", \"email\") \\\n",
					"    .withColumnRenamed(\"stat_cd\", \"statCd\") \\\n",
					"    .withColumnRenamed(\"opco_id\", \"opco\") \\\n",
					"    .withColumnRenamed(\"three_months\", \"threeMonths\") \\\n",
					"    .withColumnRenamed(\"six_months\", \"sixMonths\") \\\n",
					"    .withColumnRenamed(\"all_times\", \"allTimes\") \\\n",
					"    .withColumnRenamed(\"last_order\", \"lastOrder\") \\\n",
					"    .withColumnRenamed(\"removed_purchases\", \"removedPurchases\") \\\n",
					"    .withColumnRenamed(\"ordergenius_rank_partition\", \"ordergeniusRankPartition\") \\\n",
					"    .withColumnRenamed(\"ordergenius_default\", \"ordergeniusDefault\") \\\n",
					"    .withColumnRenamed(\"ordergenius_set_recency_decay\", \"ordergeniusSetRecencyDecay\") \\\n",
					"    .withColumnRenamed(\"first_lpurc_dt\", \"firstLpurcDt\") \\\n",
					"    .withColumnRenamed(\"last_lpurc_dt\", \"lastLpurcDt\") \\\n",
					"    .withColumnRenamed(\"last_purc_dates\", \"lastPurcDates\") \\\n",
					"    .withColumnRenamed(\"avg_qtys\", \"avgQtys\") \\\n",
					"    .withColumnRenamed(\"total_orders\", \"totalOrders\") \\\n",
					"    .withColumnRenamed(\"total_amount\", \"totalAmount\") \\\n",
					"    .withColumnRenamed(\"avg_ord_amt\", \"avgOrdAmt\") \\\n",
					"    .withColumnRenamed(\"max_tot_purc_qy\", \"maxTotPurcQy\") \\\n",
					"    .withColumnRenamed(\"category_averages\", \"categoryAverages\") \\\n",
					"    .withColumnRenamed(\"last_updated_at\", \"lastUpdatedAt\") \\\n",
					"    .withColumnRenamed(\"last_updated_by\", \"lastUpdatedBy\") \\\n",
					"    .drop(\"hash\")\n",
					"\n",
					"df_past_purc_data_store \\\n",
					"    .write \\\n",
					"    .format(\"cosmos.oltp\") \\\n",
					"    .options(**cfg) \\\n",
					"    .mode(\"append\") \\\n",
					"    .save()\n",
					"\n",
					"logger.info(\"persist: Persisted past-purchase in Cosmos for OPCO = {0}, Count={1}, Container={2}\".format(OPCO, df_past_purc.count(), COSMOS_CONTAINER_NM)) \n",
					""
				],
				"execution_count": null
			}
		]
	}
}