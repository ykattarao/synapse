{
	"name": "nb_integrate_past-purchase",
	"properties": {
		"folder": {
			"name": "integration/past-purchase"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "synsppdlinte201",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e5e4c599-b90c-4e01-95fb-a592639aaf02"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/68ab6d29-6524-4862-8fb0-8b171dcf03a9/resourceGroups/rg-synw-pdlintegrations-nonprd-dev-e2-01/providers/Microsoft.Synapse/workspaces/synw-pdlintegrations-nonprd-dev-e2-01/bigDataPools/synsppdlinte201",
				"name": "synsppdlinte201",
				"type": "Spark",
				"endpoint": "https://synw-pdlintegrations-nonprd-dev-e2-01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/synsppdlinte201",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**INITILIZATION OF VARIBLES**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"BASE_ADLS_CONN_STR = \"abfss://data-integration@sasynwpdlintnpdeve201.dfs.core.windows.net/\"\r\n",
					"TARGET_STORE_ORDER_ITEM_USER_PATH = \"opco/{opco}/domain/order/store-order/master/order-item-user/\"\r\n",
					"TARGET_WEB_ORDER_ITEM_PATH=\"opco/{opco}/domain/order/web-order/master/order-item/\"\r\n",
					"TARGET_USER_PATH = \"opco/{opco}/domain/customer/user/master/\"\r\n",
					"TARGET_REPL_PROD_PATH =\"opco/{opco}/domain/merch/product/retailer-cpt-it-rtlr-repl/master/product-id/\"\r\n",
					"TARGET_RTL_ECOM_STORE_CTL_PATH=\"opco/{opco}/domain/store/retailer-cpt-ecom-store-ctl/master/\"\r\n",
					"TARGET_DEL_PROD_PATH =\"opco/{opco}/domain/merch/product/retailer-cpt-past-purc-del/master/\"\r\n",
					"STAGE_STORE_ORDER_PATH= \"opco/{opco}/domain/integration/past-purchase/stage/store-order/\"\r\n",
					"STAGE_WEB_ORDER_PATH=\"opco/{opco}/domain/integration/past-purchase/stage/web-order\"\r\n",
					"STAGE_USER_PATH = \"opco/{opco}/domain/integration/past-purchase/stage/user\"\r\n",
					"STAGE_REPL_PROD_PATH=\"opco/{opco}/domain/integration/past-purchase/stage/replacement-products\"\r\n",
					"STAGE_DEL_PROD_PATH =\"opco/{opco}/domain/integration/past-purchase/stage/deleted-products\"\r\n",
					"STAGE_ORDER_GENIUS_DEFAULT_PATH=\"opco/{opco}/domain/integration/past-purchase/stage/order-genius-default\"\r\n",
					"OUTGOING_CUSTOMER_PATH = \"-\"\r\n",
					"OUTGOING_CUSTOMER_FL = \"-\"\r\n",
					"OUTGOING_WEB_ORDER_PATH = \"-\"\r\n",
					"OUTGOING_WEB_ORDER_FL = \"-\"\r\n",
					"FL_PATH_SEPARATOR = \"/\"\r\n",
					"TARGET_PAST_PURC_PATH=\"opco/{opco}/domain/integration/past-purchase/master\"\r\n",
					"NO_OF_PARTITIONS = 25\r\n",
					"NO_OF_PROD_ORD_RANK=500\r\n",
					"NO_OF_PARTITONS_ORD_RANK=6\r\n",
					"NO_OF_DAYS_DATA= 275\r\n",
					"IS_DELTA_LOAD = False\r\n",
					"OPCO = \"fdln\"\r\n",
					"VAL_CNT_THRESHOLD=5\r\n",
					"EXCLUDE_COLUMNS=[\"three_months_init\",\"six_months_init\",\"all_times_init\",\"last_order_init\",\"ordergenius_rank_partition\",\r\n",
					"                \"created_ts\",\"created_by\",\"last_updated_at\",\"last_updated_by\",\"hash\",\"hash_cd\",\"is_processed\",\"is_active\"]\r\n",
					"COLS_ORDER =    [\"user_id\",\"hndl_id\",\"rtlr_card_id\",\"mail_tx\",\"stat_cd\",\"opco_id\",\"ordergenius_default\",\"ordergenius_rank_partition\",\r\n",
					"                \"three_months_init\",\"six_months_init\",\"all_times_init\",\"last_order_init\",\"three_months\",\"six_months\",\"all_times\",\"last_order\",\r\n",
					"                \"removed_purchases\",\"id\",\"ordergenius_set_recency_decay\",\"first_lpurc_dt\",\"last_lpurc_dt\",\"last_purc_dates\",\"avg_qtys\",\"qtys\",\r\n",
					"                \"categories\",\"category_averages\",\"total_orders\",\"total_amount\",\"avg_ord_amt\",\"max_tot_purc_qy\",\"created_ts\",\r\n",
					"                \"created_by\",\"last_updated_by\",\"last_updated_at\",\"hash\",\"hash_cd\",\"is_processed\",\"is_active\"]                \r\n",
					"USR_PAST_PURCHASE = \"past-purchase-integration-pipeline\"\r\n",
					"COSMOS_LINKED_SERVICE_NM = \"ls_az_cosmos_core_services_purchase_history_fdln\"\r\n",
					"COSMOS_CONTAINER_NM = \"past-purchase\"\r\n",
					"COSMOS_DB_NM = \"purchase_history\""
				],
				"execution_count": 88
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**INCLUDE UTILS**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/logging/nb_logging_util { LOGGER_NM: \"nb_integrate_past-purchase\", LOGGING_LEVEL: \"INFO\" }"
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run utils/validation/nb_auto_data_validation_framework"
				],
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"logger.info(\"*********************************************************************************\")\n",
					"logger.info(\"\\t\\t Starting Past Purchase Integration with below params - {0}\".format(OPCO))\n",
					"logger.info(\"*********************************************************************************\")\n",
					"logger.info(\"\\t\\t IS_DELTA_LOAD = {0}\".format(IS_DELTA_LOAD))\n",
					"logger.info(\"\\t\\t TARGET_USER_PATH = {0}\".format(TARGET_USER_PATH))\n",
					"logger.info(\"\\t\\t TARGET_WEB_ORDER_ITEM_PATH = {0}\".format(TARGET_WEB_ORDER_ITEM_PATH))\n",
					"logger.info(\"\\t\\t TARGET_STORE_ORDER_ITEM_USER_PATH = {0}\".format(TARGET_STORE_ORDER_ITEM_USER_PATH))\n",
					"logger.info(\"\\t\\t TARGET_DEL_PROD_PATH = {0}\".format(TARGET_DEL_PROD_PATH))\n",
					"logger.info(\"\\t\\t OUTGOING_CUSTOMER_PATH = {0}\".format(OUTGOING_CUSTOMER_PATH))\n",
					"logger.info(\"\\t\\t OUTGOING_CUSTOMER_FL = {0}\".format(OUTGOING_CUSTOMER_FL))\n",
					"logger.info(\"\\t\\t OUTGOING_WEB_ORDER_PATH = {0}\".format(OUTGOING_WEB_ORDER_PATH))\n",
					"logger.info(\"\\t\\t OUTGOING_WEB_ORDER_FL = {0}\".format(OUTGOING_WEB_ORDER_FL))\n",
					"logger.info(\"\\t\\t TARGET_PAST_PURC_PATH = {0}\".format(TARGET_PAST_PURC_PATH))\n",
					"logger.info(\"\\t\\t NO_OF_PARTITIONS = {0}\".format(NO_OF_PARTITIONS))\n",
					"logger.info(\"\\t\\t NO_OF_DAYS_DATA = {0}\".format(NO_OF_DAYS_DATA))\n",
					"logger.info(\"**********************************************************************************\")"
				],
				"execution_count": 70
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**GETTING DELTA CUSTOMERS**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from delta.tables import DeltaTable\n",
					"from pyspark.sql.types import StructType, StructField, IntegerType\n",
					"\n",
					"if IS_DELTA_LOAD:\n",
					"    logger.info('delta-users: Started loading Delta Users from Customer and Web Order Domain')\n",
					"    # Create empty datafrmae for delta user\n",
					"    df_delta_usr = spark.createDataFrame([], StructType([StructField(\"user_id\", IntegerType(), False)]))\n",
					"\n",
					"    # Append delta users from customer domain\n",
					"    if OUTGOING_CUSTOMER_FL and OUTGOING_CUSTOMER_FL.strip() != '-':\n",
					"        OUTGOING_CUSTOMER_PATH = OUTGOING_CUSTOMER_PATH.split('/',1)[1]\n",
					"        df_customer_usr = spark \\\n",
					"            .read \\\n",
					"            .option(\"inferSchema\", \"true\") \\\n",
					"            .option(\"header\", \"true\") \\\n",
					"            .csv(BASE_ADLS_CONN_STR + OUTGOING_CUSTOMER_PATH + FL_PATH_SEPARATOR + OUTGOING_CUSTOMER_FL) \\\n",
					"            .select(\"user_id\")        \n",
					"        df_delta_usr = df_delta_usr.unionByName(df_customer_usr)\n",
					"        logger.info(\"delta-users: Loaded Delta Users from Customer. OPCO={0}, Count={1}, Path={2}\".format(OPCO, df_customer_usr.count(), OUTGOING_CUSTOMER_PATH))\n",
					"\n",
					"    # Append delta users from web order domain\n",
					"    if OUTGOING_WEB_ORDER_FL and OUTGOING_WEB_ORDER_FL.strip() != '-':\n",
					"        OUTGOING_WEB_ORDER_PATH = OUTGOING_WEB_ORDER_PATH.split('/',1)[1]\n",
					"        df_web_ord_usr = spark \\\n",
					"            .read \\\n",
					"            .option(\"inferSchema\", \"true\") \\\n",
					"            .option(\"header\", \"true\") \\\n",
					"            .csv(BASE_ADLS_CONN_STR + OUTGOING_WEB_ORDER_PATH + FL_PATH_SEPARATOR + OUTGOING_WEB_ORDER_FL) \\\n",
					"            .withColumnRenamed(\"userId\", \"user_id\") \\\n",
					"            .select(\"user_id\") \\\n",
					"            .distinct()\n",
					"        df_delta_usr = df_delta_usr.unionByName(df_web_ord_usr)\n",
					"        logger.info(\"delta-users: Loaded Delta Users from Web Order Domain. OPCO={0}, Count={1}, Path={2}\".format(OPCO, df_web_ord_usr.count(), OUTGOING_WEB_ORDER_PATH))\n",
					"    \n",
					"    df_delta_usr = df_delta_usr.dropDuplicates()\n",
					"    logger.info(\"delta-users: Loaded unique Delta Users from Customer and Web Order Domain. OPCO={0}, Count={1}\".format(OPCO, df_delta_usr.count()))"
				],
				"execution_count": 71
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD STORE ORDER DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from delta.tables import DeltaTable\r\n",
					"from pyspark.sql import functions as f\r\n",
					"from pyspark.sql.window import Window\r\n",
					"\r\n",
					"def load_delta_toDF(path):\r\n",
					"\r\n",
					"    df_delta_load = DeltaTable.forPath(spark, path)\r\n",
					"    df_load=df_delta_load.toDF()\r\n",
					"    \r\n",
					"    logger.info(\"\\t read-delta-table:Loading Delta file and converting them to DF.\\n OPCO = {0},\\n Path={1},Count={2}\".format(OPCO,path,df_load.count()))\r\n",
					"    return df_load\r\n",
					"\r\n",
					"def write_stage_stor_ord(df_stg):\r\n",
					"    df_stg \\\r\n",
					"        .repartition(1, \"user_id\") \\\r\n",
					"        .sort(\"user_id\") \\\r\n",
					"        .write \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"hash\") \\\r\n",
					"        .save(BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH.format(opco = OPCO))\r\n",
					"    logger.info(\"write_stage_stor_ord : Write store orders into STG container. OPCO = {0}, Count = {1}, Path = {2}\".format(OPCO, df_stg.count(), STAGE_STORE_ORDER_PATH))\r\n",
					"\r\n",
					"def get_delta_stor_ord(df_mstr, df_delta):\r\n",
					"    df_delta_stor_ord = df_mstr \\\r\n",
					"        .join(df_delta, df_mstr[\"user_id\"] == df_delta[\"user_id\"], \"inner\") \\\r\n",
					"        .select(df_mstr[\"*\"])\r\n",
					"    logger.info(\"get_delta_stor_ord : Filter DELTA store orders. OPCO = {0}, Count = {1}\".format(OPCO, df_delta_stor_ord.count()))\r\n",
					"    return df_delta_stor_ord\r\n",
					"\r\n",
					"#main starts here \r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH.format(opco = OPCO))):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH.format(opco = OPCO), recurse = True)\r\n",
					"    logger.info('Purging existing stage store order data')\r\n",
					"\r\n",
					"\r\n",
					"if IS_DELTA_LOAD:\r\n",
					"    df_stor_ord_master = load_delta_toDF(BASE_ADLS_CONN_STR + TARGET_STORE_ORDER_ITEM_USER_PATH.format(opco = OPCO))\r\n",
					"    df_stor_ord_tobe_processed = get_delta_stor_ord(df_stor_ord_master, df_delta_usr)\r\n",
					"    write_stage_stor_ord(df_stor_ord_tobe_processed)\r\n",
					"else:\r\n",
					"    STAGE_STORE_ORDER_PATH=TARGET_STORE_ORDER_ITEM_USER_PATH\r\n",
					""
				],
				"execution_count": 72
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD WEB DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from delta.tables import DeltaTable\r\n",
					"from pyspark.sql import functions as f\r\n",
					"\r\n",
					"def get_all_web_ord():\r\n",
					"    curr_date= f.current_date()\r\n",
					"    from_date = curr_date - f.expr(\"INTERVAL \"+ str(NO_OF_DAYS_DATA) +\" DAYS\")\r\n",
					"    df_web_order_mstr = load_delta_toDF(BASE_ADLS_CONN_STR + TARGET_WEB_ORDER_ITEM_PATH.format(opco = OPCO))\r\n",
					"    df_web_order_mstr=df_web_order_mstr.filter((df_web_order_mstr.orderDate >= from_date) & (df_web_order_mstr.orderDate<= curr_date) ) \\\r\n",
					"                                        .filter(\"userId IS NOT NULL\") \\\r\n",
					"                                        .withColumn(\"hash\", f.abs(f.hash(f.col(\"userId\"))%NO_OF_PARTITIONS)) \\\r\n",
					"                                        .withColumnRenamed(\"orderId\", \"order_id\") \\\r\n",
					"                                        .withColumnRenamed(\"orderDate\", \"order_date\") \\\r\n",
					"                                        .withColumnRenamed(\"orderTime\", \"order_time\") \\\r\n",
					"                                        .withColumnRenamed(\"deliveryDate\", \"delivery_date\") \\\r\n",
					"                                        .withColumnRenamed(\"userId\", \"user_id\") \\\r\n",
					"                                        .withColumnRenamed(\"podId\", \"pod_id\") \\\r\n",
					"                                        .withColumnRenamed(\"ordQy\",\"ord_Qy\")\\\r\n",
					"                                        .select(\"order_id\", \"order_date\", \"delivery_date\", \"user_id\", \"pod_id\",\"ord_Qy\",\"hash\",\"order_time\")\r\n",
					"    logger.info(\"get_all_web_ord : Extract ALL web orders between {0} and {1}. OPCO = {2}, Count = {3}\".format(from_date, curr_date, OPCO, df_web_order_mstr.count()))\r\n",
					"    return df_web_order_mstr\r\n",
					"\r\n",
					"def get_delta_web_ord(df_mstr, df_delta):\r\n",
					"    df_delta_web_ord = df_mstr \\\r\n",
					"        .join(df_delta, df_mstr[\"user_id\"] == df_delta[\"user_id\"], \"inner\") \\\r\n",
					"        .select(df_mstr[\"*\"])\r\n",
					"    logger.info(\"get_delta_web_ord : Filter DELTA web orders. OPCO = {0}, Count = {1}\".format(OPCO, df_delta_web_ord.count()))\r\n",
					"    return df_delta_web_ord\r\n",
					"\r\n",
					"def write_stage_web_ord(df_stg):\r\n",
					"    df_stg \\\r\n",
					"        .repartition(1, \"user_id\") \\\r\n",
					"        .sort(\"user_id\") \\\r\n",
					"        .write \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"hash\") \\\r\n",
					"        .save(BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH.format(opco = OPCO))\r\n",
					"    logger.info(\"write_stage_web_ord : Write web orders into STG container. OPCO = {0}, Count = {1}, Path = {2}\".format(OPCO, df_stg.count(), STAGE_WEB_ORDER_PATH))\r\n",
					"\r\n",
					"# main starts here\r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH.format(opco = OPCO))):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH.format(opco = OPCO), recurse = True)\r\n",
					"    logger.info(\"Purging staging data\")\r\n",
					"\r\n",
					"df_web_ord_master = get_all_web_ord()\r\n",
					"\r\n",
					"if IS_DELTA_LOAD:\r\n",
					"    df_web_ord_tobe_processed = get_delta_web_ord(df_web_ord_master, df_delta_usr)\r\n",
					"    write_stage_web_ord(df_web_ord_tobe_processed)\r\n",
					"else:\r\n",
					"    write_stage_web_ord(df_web_ord_master)\r\n",
					""
				],
				"execution_count": 73
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD CUSTOMER DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as f\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"def get_all_users():\r\n",
					"    df_usr_mstr = load_delta_toDF(BASE_ADLS_CONN_STR + TARGET_USER_PATH.format(opco = OPCO))\r\n",
					"    df_usr_mstr=df_usr_mstr.filter(f.col(\"rtlr_card_id\").isNotNull())\\\r\n",
					"                            .filter(f.col(\"rtlr_card_id\")!=\"\")\r\n",
					"    \r\n",
					"    df_usr_mstr=    df_usr_mstr.withColumn(\"hash\", f.abs(f.hash(f.col(\"user_id\"))%NO_OF_PARTITIONS))\r\n",
					"\r\n",
					"    logger.info(\"get_all_users : Extract ALL customers. OPCO = {0}, Count = {1}\".format(OPCO, df_usr_mstr.count()))\r\n",
					"    return df_usr_mstr\r\n",
					"\r\n",
					"def get_delta_users(df_mstr, df_delta):\r\n",
					"    df_delta = df_delta \\\r\n",
					"        .withColumn(\"hash\", f.abs(f.hash(f.col(\"user_id\"))%NO_OF_PARTITIONS))\r\n",
					"    df_user_tobe_processed = df_mstr \\\r\n",
					"        .join(df_delta, (df_mstr.user_id == df_delta.user_id) & (df_mstr.hash == df_delta.hash), \"inner\") \\\r\n",
					"        .select(df_mstr[\"*\"])\r\n",
					"    logger.info(\"get_delta_users : Filter DELTA customers. OPCO = {0}, Count = {1}\".format(OPCO, df_user_tobe_processed.count()))\r\n",
					"    return df_user_tobe_processed\r\n",
					"\r\n",
					"def write_stage_user(df_stg):\r\n",
					"    df_stg \\\r\n",
					"        .repartition(1, \"user_id\") \\\r\n",
					"        .sort(\"user_id\") \\\r\n",
					"        .write \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"hash\") \\\r\n",
					"        .save(BASE_ADLS_CONN_STR + STAGE_USER_PATH.format(opco = OPCO))\r\n",
					"    logger.info(\"write_stage_user : Write customers into STG container. OPCO = {0}, Count = {1}, Path = {2}\".format(OPCO, df_stg.count(), STAGE_USER_PATH))\r\n",
					"\r\n",
					"#main starts here\r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_USER_PATH.format(opco = OPCO))):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_USER_PATH.format(opco = OPCO), recurse = True)\r\n",
					"    logger.info('Purging customer staging data')\r\n",
					"\r\n",
					"df_users_master = get_all_users()\r\n",
					"\r\n",
					"if IS_DELTA_LOAD:\r\n",
					"    df_users_tobe_processed = get_delta_users(df_users_master, df_delta_usr)\r\n",
					"    write_stage_user(df_users_tobe_processed)\r\n",
					"else:\r\n",
					"    write_stage_user(df_users_master)"
				],
				"execution_count": 74
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD REPLACEMENT PRODUCTS DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH.format(opco = OPCO))):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH.format(opco = OPCO), recurse = True)\r\n",
					"    logger.info(\"replcamen-product: Purging repalcement products from STG container. OPCO={0}\".format(OPCO))\r\n",
					"\r\n",
					"df_repl_store_prod = load_delta_toDF(BASE_ADLS_CONN_STR + TARGET_REPL_PROD_PATH.format(opco = OPCO))\r\n",
					"df_ecom_store_ctl=load_delta_toDF(BASE_ADLS_CONN_STR +TARGET_RTL_ECOM_STORE_CTL_PATH.format(opco = OPCO))\r\n",
					"\r\n",
					"df_repl_prod=df_repl_store_prod.join(df_ecom_store_ctl,on=\"ecom_stor_id\",how=\"inner\")\\\r\n",
					"                                .filter(f.col(\"prod_id\").isNotNull())\\\r\n",
					"                                .filter(f.size(df_repl_store_prod.replacement_prod_ids) >0)\\\r\n",
					"                                .filter(df_ecom_store_ctl.opco_id=='{0}'.format(OPCO.upper()))\\\r\n",
					"                                .select(\"ecom_stor_id\",\"prod_id\",\"replacement_prod_ids\",\"opco_id\")\r\n",
					"                           \r\n",
					"df_repl_prod\\\r\n",
					"        .repartition(1,\"prod_id\") \\\r\n",
					"        .write \\\r\n",
					"        .format(\"delta\") \\\r\n",
					"        .mode(\"overwrite\") \\\r\n",
					"        .partitionBy(\"ecom_stor_id\") \\\r\n",
					"        .save(BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH.format(opco = OPCO))\r\n",
					"logger.info(\"replcamen-product: Write repalcement-products into STG container. OPCO = {0}, Count = {1}\".format(OPCO, df_repl_prod.count()))"
				],
				"execution_count": 75
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**STAGE LOAD DELETED PRODUCTS DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import functions as f\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH.format(opco = OPCO))):\r\n",
					"    mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH.format(opco = OPCO), recurse = True)\r\n",
					"    logger.info(\"deleted-product: Purging deleted products from STG container. OPCO = {0}\".format(OPCO))\r\n",
					"\r\n",
					"\r\n",
					"df_del_prod=load_delta_toDF(BASE_ADLS_CONN_STR + TARGET_DEL_PROD_PATH.format(opco = OPCO))\r\n",
					"\r\n",
					"df_del_prod=df_del_prod.withColumn(\"hash\", f.abs(f.hash(f.col(\"user_id\"))%NO_OF_PARTITIONS))\r\n",
					"                            \r\n",
					"df_del_prod\\\r\n",
					"    .repartition(1, \"user_id\") \\\r\n",
					"    .sort(\"user_id\") \\\r\n",
					"    .write \\\r\n",
					"    .format(\"delta\") \\\r\n",
					"    .mode(\"overwrite\") \\\r\n",
					"    .partitionBy(\"hash\") \\\r\n",
					"    .save(BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH.format(opco = OPCO))\r\n",
					"logger.info(\"deleted-product: Write deleted-products into STG container. OPCO = {0}, Count = {1}\".format(OPCO, df_del_prod.count()))"
				],
				"execution_count": 76
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**LOAD DATA FROM STAGE**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from delta.tables import DeltaTable\r\n",
					"from pyspark.sql import functions as f\r\n",
					"import datetime\r\n",
					"from pyspark.sql.types import StringType, ArrayType, DecimalType, IntegerType,MapType\r\n",
					"\r\n",
					"def check_if_stage_file_exists():\r\n",
					"    return DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH.format(opco = OPCO)) & \\\r\n",
					"           DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH.format(opco = OPCO)) & \\\r\n",
					"           DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_USER_PATH.format(opco = OPCO)) & \\\r\n",
					"           DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH.format(opco = OPCO)) & \\\r\n",
					"           DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH.format(opco = OPCO))\r\n",
					"\r\n",
					"\r\n",
					"is_stage_fl_exist = check_if_stage_file_exists()\r\n",
					"if is_stage_fl_exist == False:  \r\n",
					"    logger.error(\"check_if_stage_file_exists: Some of the input files don't exist in STG container. OPCO={0}. Check the below input paths \\n. {1}\\n{2}\\n{3}\\n{4}\\n{5}\".format(\r\n",
					"        OPCO,\r\n",
					"        STAGE_STORE_ORDER_PATH,\r\n",
					"        STAGE_WEB_ORDER_PATH,\r\n",
					"        STAGE_USER_PATH,\r\n",
					"        STAGE_REPL_PROD_PATH,\r\n",
					"        STAGE_DEL_PROD_PATH\r\n",
					"    ))\r\n",
					"    raise Exception(\"check_if_stage_file_exists: Some of the input files don't exist in STG container.\")\r\n",
					"\r\n",
					"#tbl_store_order\r\n",
					"\r\n",
					"df_store_order=load_delta_toDF(BASE_ADLS_CONN_STR + STAGE_STORE_ORDER_PATH.format(opco = OPCO))\r\n",
					"df_store_order=df_store_order.select(\"transaction_ts\", \"user_id\", \"pod_id\",\"loyalty_id\",\"qty_sld_amt\")\r\n",
					"df_store_order.createOrReplaceTempView(\"tbl_store_order\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_store_order] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_store_order.count()))\r\n",
					"\r\n",
					"#tbl_web_order\r\n",
					"df_web_order=load_delta_toDF(BASE_ADLS_CONN_STR + STAGE_WEB_ORDER_PATH.format(opco = OPCO))\r\n",
					"df_web_order=df_web_order\\\r\n",
					"                .withColumn(\"order_date_time\",f.to_timestamp(f.concat(f.to_date(f.col(\"order_date\"),\"yyyy-MM-dd\").cast(\"string\"), f.lit(\" \"), f.col(\"order_time\")), \"yyyy-MM-dd HH:mm:ss\"))\\\r\n",
					"                .withColumn(\"delivery_date_time\",f.to_timestamp(f.concat(f.to_date(f.col(\"delivery_date\"),\"yyyy-MM-dd\").cast(\"string\"),f.lit(\" \"),f.lit(\"00:00:00\")), \"yyyy-MM-dd HH:mm:ss\"))\\\r\n",
					"                .select(\"order_time\",\"order_date\", \"delivery_date\", \"user_id\", \"pod_id\",  \"order_date_time\", \"delivery_date_time\",\"ord_Qy\")\r\n",
					"df_web_order.createOrReplaceTempView(\"tbl_web_order\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_web_order] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_web_order.count()))\r\n",
					"\r\n",
					"#tbl_repl_prod_id\r\n",
					"\r\n",
					"df_repl_prod_id=load_delta_toDF(BASE_ADLS_CONN_STR + STAGE_REPL_PROD_PATH.format(opco = OPCO))\r\n",
					"\r\n",
					"df_repl_prod_id.createOrReplaceTempView(\"tbl_repl_prod_id\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_repl_prod_id] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_repl_prod_id.count()))\r\n",
					"\r\n",
					"#tbl_users\r\n",
					"\r\n",
					"df_users=load_delta_toDF(BASE_ADLS_CONN_STR + STAGE_USER_PATH.format(opco = OPCO))\r\n",
					"df_users=df_users.select(\"user_id\", \"hndl_id\", \"rtlr_card_id\", \"mail_tx\", \"stat_cd\", \"opco_id\")\r\n",
					"df_users.createOrReplaceTempView(\"tbl_users\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_users] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_users.count()))\r\n",
					"\r\n",
					"#tbl_deleted_prod\r\n",
					"\r\n",
					"df_deleted_prod=load_delta_toDF(BASE_ADLS_CONN_STR + STAGE_DEL_PROD_PATH.format(opco = OPCO))    \r\n",
					"df_deleted_prod.createOrReplaceTempView(\"tbl_deleted_prod\")\r\n",
					"logger.info(\"stage-data-load: Created Stage table[tbl_deleted_prod] for all domains. Ready to start the integration. OPCO={0}, Path={1}, Count={2}\".format(OPCO, STAGE_STORE_ORDER_PATH, df_deleted_prod.count()))\r\n",
					"\r\n",
					"logger.info(\"stage-data-load: Created required all stage tables for all domains. Ready to start the integration. OPCO={0}\".format(OPCO))"
				],
				"execution_count": 77
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**JOIN STORE WEB DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_union_query = spark.sql(\r\n",
					"    \"\"\"\r\n",
					"    SELECT transaction_ts as order_date,user_id,pod_id,qty_sld_amt as ord_Qy\r\n",
					"    FROM tbl_store_order\r\n",
					"    UNION ALL\r\n",
					"    SELECT delivery_date_time as order_date,user_id,pod_id,ord_Qy\r\n",
					"    FROM tbl_web_order\r\n",
					"    \"\"\"\r\n",
					"    )\r\n",
					"df_union_query.createOrReplaceTempView(\"tbl_web_store_order\")\r\n",
					"logger.info(\"merge-web-store-orders: Merged store and web orders into a single dataset. OPCO = {0}\".format(OPCO)) "
				],
				"execution_count": 78
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**SPLITTING WEB STORE DATA INTO 3 MONTHS, 6 MONTHS & ALL TIMES**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"logger.info(\"aggregate-products: Aggregate 3Months, 6Months and AllTime Products. OPCO = {0}\".format(OPCO)) \r\n",
					"df_query = spark.sql(\"\"\"\r\n",
					"                        SELECT  a.user_id,\r\n",
					"                        a.pod_id,\r\n",
					"                        array_distinct(concat(array(a.pod_id), coalesce(b.replacement_prod_ids,array()))) repl_prod_ids_lst,b.ecom_stor_id,                     \r\n",
					"                        CASE WHEN months_between(current_date(),a.order_date)<=3 THEN TRUE ELSE FALSE END as orderWindowThreeMonths,\r\n",
					"                        CASE WHEN months_between(current_date(),a.order_date)<=6 THEN TRUE ELSE FALSE END as orderWindowSixMonths,\r\n",
					"                        CASE WHEN months_between(current_date(),a.order_date)<=36 THEN TRUE ELSE FALSE END as orderWindowAllTimes\r\n",
					"                        FROM tbl_web_store_order a\r\n",
					"                        LEFT JOIN tbl_repl_prod_id b ON a.pod_id=b.prod_id\r\n",
					"                    \"\"\")\r\n",
					"                \r\n",
					"\r\n",
					"df_usr_prd_three_months = df_query \\\r\n",
					"                                .where(f.col(\"orderWindowThreeMonths\") == True) \\\r\n",
					"                                .groupBy(\"user_id\") \\\r\n",
					"                                .agg(f.array_sort(f.array_distinct(f.flatten(f.collect_set(\"repl_prod_ids_lst\")))).alias(\"three_months\")) \\\r\n",
					"                                .repartition(100, \"user_id\")\r\n",
					"\r\n",
					"df_usr_prd_three_months =df_usr_prd_three_months.withColumnRenamed(\"user_id\",\"user_id_three_months\")\r\n",
					"\r\n",
					"\r\n",
					"df_usr_prd_six_months = df_query \\\r\n",
					"                                .where(f.col(\"orderWindowSixMonths\") == True) \\\r\n",
					"                                .groupBy(\"user_id\") \\\r\n",
					"                                .agg(f.array_sort(f.array_distinct(f.flatten(f.collect_set(\"repl_prod_ids_lst\")))).alias(\"six_months\"))  \\\r\n",
					"                                .repartition(100, \"user_id\")\r\n",
					"\r\n",
					"df_usr_prd_six_months =df_usr_prd_six_months.withColumnRenamed(\"user_id\",\"user_id_six_months\")   \r\n",
					"                                          \r\n",
					"\r\n",
					"df_usr_prd_all_times = df_query \\\r\n",
					"                                .where(f.col(\"orderWindowAllTimes\") == True) \\\r\n",
					"                                .groupBy(\"user_id\") \\\r\n",
					"                                .agg(f.array_sort(f.array_distinct(f.flatten(f.collect_set(\"repl_prod_ids_lst\")))).alias(\"all_times\"))  \\\r\n",
					"                                .repartition(100, \"user_id\")\r\n",
					"\r\n",
					"df_usr_prd_all_times=df_usr_prd_all_times.withColumnRenamed(\"user_id\",\"user_id_all\")"
				],
				"execution_count": 79
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**LAST ORDER DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"logger.info(\"aggregate-products: Aggregate LatestOrder Products. OPCO = {0}\".format(OPCO)) \r\n",
					"\r\n",
					"df_last_order_query = spark.sql(\"\"\"\r\n",
					"                        SELECT\r\n",
					"\t\t                a.user_id,\t\t                \r\n",
					"\t\t                a.pod_id,\r\n",
					"                        array_distinct(concat(array(a.pod_id), coalesce(c.replacement_prod_ids,array()))) pod_id_lst\r\n",
					"                    FROM tbl_web_store_order a\r\n",
					"                        INNER JOIN\r\n",
					"                        (\r\n",
					"                            SELECT user_id,MAX(order_date) last_ordered_time\r\n",
					"                            FROM tbl_web_store_order\r\n",
					"                            GROUP BY user_id\r\n",
					"                        ) b ON a.user_id=b.user_id AND a.order_date=b.last_ordered_time\r\n",
					"                        LEFT JOIN tbl_repl_prod_id c ON a.pod_id=c.prod_id\r\n",
					"                    \"\"\")\r\n",
					"\r\n",
					"df_usr_prd_last_order = df_last_order_query \\\r\n",
					"                                .groupBy(\"user_id\") \\\r\n",
					"                                .agg(f.array_sort(f.array_distinct(f.flatten(f.collect_set(\"pod_id_lst\")))).alias(\"last_order\")) \\\r\n",
					"                                .repartition(100, \"user_id\")\r\n",
					"\r\n",
					"df_usr_prd_last_order = df_usr_prd_last_order.withColumnRenamed(\"user_id\",\"user_id_last_order\") \r\n",
					""
				],
				"execution_count": 80
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**REMOVED PURCHASES DATA**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\r\n",
					"logger.info(\"aggregate-products: Remove user deleted past-purchase products. OPCO = {0}\".format(OPCO)) \r\n",
					"\r\n",
					"df_removed_purc=spark.sql(\r\n",
					"                        \"\"\"\r\n",
					"                        SELECT DISTINCT\r\n",
					"                        a.user_id as user_id_del,pod_id,c.ecom_stor_id,\r\n",
					"                        array_distinct(concat(array(a.pod_id), coalesce(c.replacement_prod_ids,array()))) as removed_repl_prod_purchases\r\n",
					"                        FROM (SELECT user_id, pod_id, MAX(order_date) max_order_date FROM tbl_web_store_order GROUP BY user_id, pod_id) a\r\n",
					"                        INNER JOIN\r\n",
					"                        tbl_deleted_prod b ON a.user_id=b.user_id AND a.pod_id=b.prod_id AND a.max_order_date < b.audt_cr_dt_tm\r\n",
					"                        LEFT JOIN\r\n",
					"                        tbl_repl_prod_id c ON b.prod_id=c.prod_id\r\n",
					"                        \"\"\")\r\n",
					"\r\n",
					"df_removed_purc = df_removed_purc.groupBy(\"user_id_del\") \\\r\n",
					"                                .agg(f.array_sort(f.array_distinct(f.flatten(f.collect_set(\"removed_repl_prod_purchases\")))).alias(\"removed_purchases\"))\r\n",
					"                          \r\n",
					"df_usr_prd_three_months = df_usr_prd_three_months \\\r\n",
					"    .join(df_removed_purc, df_usr_prd_three_months.user_id_three_months == df_removed_purc.user_id_del, \"left\") \\\r\n",
					"    .withColumn(\"three_months_purch_removed\", \\\r\n",
					"        F.when(F.size(df_removed_purc.removed_purchases) > 0, F.array_except(df_usr_prd_three_months.three_months, df_removed_purc.removed_purchases)) \\\r\n",
					"            .otherwise(df_usr_prd_three_months.three_months)) \\\r\n",
					"    .drop(\"user_id_del\",\"removed_purchases\")\r\n",
					"logger.info(\"aggregate-products: Remove user deleted past-purchase products from df_usr_prd_three_months. OPCO = {0}\".format(OPCO)) \r\n",
					"\r\n",
					"df_usr_prd_six_months = df_usr_prd_six_months \\\r\n",
					"    .join(df_removed_purc, df_usr_prd_six_months.user_id_six_months == df_removed_purc.user_id_del, \"left\") \\\r\n",
					"    .withColumn(\"six_months_purch_removed\", \\\r\n",
					"        F.when(F.size(df_removed_purc.removed_purchases) > 0, F.array_except(df_usr_prd_six_months.six_months, df_removed_purc.removed_purchases)) \\\r\n",
					"            .otherwise(df_usr_prd_six_months.six_months)) \\\r\n",
					"    .drop(\"user_id_del\",\"removed_purchases\")\r\n",
					"logger.info(\"aggregate-products: Remove user deleted past-purchase products from df_usr_prd_six_months. OPCO = {0}\".format(OPCO)) \r\n",
					"\r\n",
					"df_usr_prd_all_times = df_usr_prd_all_times \\\r\n",
					"    .join(df_removed_purc, df_usr_prd_all_times.user_id_all == df_removed_purc.user_id_del, \"left\") \\\r\n",
					"    .withColumn(\"all_times_purch_removed\", \\\r\n",
					"        F.when(F.size(df_removed_purc.removed_purchases) > 0, F.array_except(df_usr_prd_all_times.all_times, df_removed_purc.removed_purchases)) \\\r\n",
					"            .otherwise(df_usr_prd_all_times.all_times)) \\\r\n",
					"    .drop(\"user_id_del\",\"removed_purchases\")\r\n",
					"logger.info(\"aggregate-products: Remove user deleted past-purchase products from df_usr_prd_all_times. OPCO = {0}\".format(OPCO)) \r\n",
					"\r\n",
					"df_usr_prd_last_order = df_usr_prd_last_order \\\r\n",
					"    .join(df_removed_purc, df_usr_prd_last_order.user_id_last_order == df_removed_purc.user_id_del, \"left\") \\\r\n",
					"    .withColumn(\"last_order_purch_removed\", \\\r\n",
					"        F.when(F.size(df_removed_purc.removed_purchases) > 0, F.array_except(df_usr_prd_last_order.last_order, df_removed_purc.removed_purchases)) \\\r\n",
					"            .otherwise(df_usr_prd_last_order.last_order)) \\\r\n",
					"    .drop(\"user_id_del\",\"removed_purchases\")\r\n",
					"logger.info(\"aggregate-products: Remove user deleted past-purchase products from df_usr_prd_last_order. OPCO = {0}\".format(OPCO)) "
				],
				"execution_count": 81
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**CALCULATING ORDER GENIUS DEFAULT,ORDER RANKING**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.types import StringType, ArrayType,MapType,NullType\r\n",
					"from pyspark.sql.functions import udf,datediff,col,countDistinct,date_format,current_timestamp,lit,collect_list\r\n",
					"from pyspark.sql.functions import max,min,sum,round,greatest,least,desc,asc,row_number\r\n",
					"from pyspark.sql.window import Window\r\n",
					"from math import ceil\r\n",
					"logger.info(\"Calculating Order genius and Order Ranking for Users. OPCO = {0}\".format(OPCO))\r\n",
					"\r\n",
					"\r\n",
					"def ord_rank_partion(ord_gen_list,max_num_prod,max_num_partitions):\r\n",
					"    \r\n",
					"    partition_size= max_num_prod//max_num_partitions\r\n",
					"    num_ord_gen_list_part=ceil(len(ord_gen_list) / partition_size)\r\n",
					"\r\n",
					"    if(max_num_partitions>num_ord_gen_list_part):        \r\n",
					"        num_partitions=num_ord_gen_list_part\r\n",
					"    else:\r\n",
					"        num_partitions=max_num_partitions   \r\n",
					"   \r\n",
					"    result = {}     \r\n",
					"    for i in range(num_partitions):\r\n",
					"        if(i+1==num_partitions):\r\n",
					"            result[str(i)] = ord_gen_list[i*partition_size:max_num_prod]            \r\n",
					"        else:\r\n",
					"            result[str(i)] = ord_gen_list[i*partition_size: (i+1)*partition_size]\r\n",
					"\r\n",
					"    return result    \r\n",
					"udf_ord_rank_partition=udf(ord_rank_partion,MapType(StringType(),ArrayType(IntegerType())))\r\n",
					"\r\n",
					"logger.info(\"User defined function to get Order Genius Rank Partition .OPCO = {0} limiting to Top {1} and No of Partions {2} \" .format(OPCO,NO_OF_PROD_ORD_RANK,NO_OF_PARTITONS_ORD_RANK))\r\n",
					"\r\n",
					"#Assining Data frame from Main Data frame\r\n",
					"df_web_store_order_union=df_union_query\r\n",
					"\r\n",
					"logger.info(\"merge-web-store-orders: Merged store and web orders into a single dataset with reguired columns . OPCO = {0}  Count = {1}\" .format(OPCO,df_web_store_order_union.count()))\r\n",
					"\r\n",
					"avg_order_feq_expr=(datediff(max(col(\"order_date\")),min(col(\"order_date\")))+1)/(countDistinct(date_format(col(\"order_date\"),\"yyyy-MM-dd\")))\r\n",
					"\r\n",
					"df_user_purchase=df_web_store_order_union.groupby(\"user_id\").agg(\r\n",
					"                                        min(col(\"order_date\")).alias(\"first_user_pur_date\"),\r\n",
					"                                        max(col(\"order_date\")).alias(\"last_user_pur_date\"),\r\n",
					"                                        sum(col(\"ord_Qy\")).alias(\"total_user_qty\"),\r\n",
					"                                        max(col(\"ord_Qy\")).alias(\"max_user_prc_qty\"),\r\n",
					"                                        datediff(current_timestamp(),min(col(\"order_date\"))).alias(\"days_since_last_first_pur\"))\r\n",
					"\r\n",
					"logger.info(\"Creating Data frame for User Purchase  . OPCO = {0} \".format(OPCO))                                       \r\n",
					"\r\n",
					"\r\n",
					"df_user_purchase_product=df_web_store_order_union.groupby(\"user_id\",\"pod_id\").agg(\r\n",
					"                                            min(col(\"order_date\")).alias(\"first_prd_pur_date\"),\r\n",
					"                                            max(col(\"order_date\")).alias(\"last_prd_pur_date\"),\r\n",
					"                                            countDistinct(date_format(col(\"order_date\"),\"yyyy-MM-dd\")).alias(\"total_unique_ords\"),\r\n",
					"                                            sum(col(\"ord_Qy\")).alias(\"total_prd_qty\"),\r\n",
					"                                            max(col(\"ord_Qy\")).alias(\"max_user_prd_qty\"),\r\n",
					"                                            round(avg_order_feq_expr,1).alias(\"avg_ord_freq_days\"),\r\n",
					"                                            datediff(current_timestamp(),max(col(\"order_date\"))).alias(\"days_since_last_pur_prd\"))\r\n",
					"\r\n",
					"logger.info(\"Creating Data frame for User Product Purchase  . OPCO = {0} \".format(OPCO))                                             \r\n",
					"               \r\n",
					"df_user_product_joined=df_user_purchase.alias(\"a\").join(df_user_purchase_product.alias(\"b\"),[\"user_id\"],how=\"inner\")\r\n",
					"\r\n",
					"logger.info(\"Joining data frame for User Purchase, Product Purchase  . OPCO = {0} \".format(OPCO)) \r\n",
					"\r\n",
					"\r\n",
					"df_recency_factor_expr=(greatest(lit(1.0),(col(\"days_since_last_first_pur\")-col(\"days_since_last_pur_prd\"))))/(greatest(lit(1.0),(col(\"days_since_last_first_pur\"))))\r\n",
					"df_pur_quan_factor_expr=(greatest(lit(1.0),col(\"total_prd_qty\")))/(greatest(lit(1.0),col(\"total_user_qty\")))\r\n",
					"df_ord_freq_factor_expr=least((greatest(lit(1.0),(col(\"days_since_last_pur_prd\")))/col(\"avg_ord_freq_days\")),(col(\"avg_ord_freq_days\")/greatest(lit(1.0),(col(\"days_since_last_pur_prd\")))))\r\n",
					"df_score_expr=pow(col(\"recency_factor\"),1)*pow(col(\"pur_qty_factor\"),1)*pow(col(\"ord_freq_factor\"),1)\r\n",
					"\r\n",
					"\r\n",
					"df_user_product_factors=df_user_product_joined.withColumn(\"recency_factor\",round(df_recency_factor_expr,4))\\\r\n",
					"                                            .withColumn(\"pur_qty_factor\",round(df_pur_quan_factor_expr,4))\\\r\n",
					"                                            .withColumn(\"ord_freq_factor\",round(df_ord_freq_factor_expr,4))\r\n",
					"\r\n",
					"logger.info(\"Order Genius and Default:Calcualting Recency factor,Purchase Quanity factor,Order Frequency factor . OPCO = {0} \".format(OPCO))\r\n",
					"\r\n",
					"df_pod_score=Window.partitionBy(\"user_id\").orderBy(desc(\"score\"),desc(\"pod_id\"))\r\n",
					"\r\n",
					"\r\n",
					"df_user_product_org_default=df_user_product_factors.withColumn(\"score\",df_score_expr)\\\r\n",
					"                                                    .withColumn(\"rank\",row_number().over(df_pod_score))\\\r\n",
					"                                                    .select(col(\"user_id\").alias(\"ord_gen_user_id\"),\"pod_id\",\"recency_factor\",\"pur_qty_factor\",\"ord_freq_factor\",\"score\",\"rank\")\r\n",
					"\r\n",
					"logger.info(\"Arranging pod_id's according to score and rank . OPCO = {0}\".format(OPCO))\r\n",
					"\r\n",
					"if(LOGGING_LEVEL== \"DEBUG\"):\r\n",
					"    if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + STAGE_ORDER_GENIUS_DEFAULT_PATH.format(opco = OPCO))):\r\n",
					"        mssparkutils.fs.rm(BASE_ADLS_CONN_STR + STAGE_ORDER_GENIUS_DEFAULT_PATH.format(opco = OPCO), recurse = True)\r\n",
					"        logger.debug(\"organic default:Purging organic default from STG container. OPCO = {0}\".format(OPCO))\r\n",
					"        \r\n",
					"    df_ord_genius_stg=df_user_product_org_default.withColumn(\"hash\", f.abs(f.hash(f.col(\"ord_gen_user_id\"))%NO_OF_PARTITIONS))\r\n",
					"    \r\n",
					"    df_ord_genius_stg.repartition(1, \"ord_gen_user_id\") \\\r\n",
					"                        .sort(\"ord_gen_user_id\") \\\r\n",
					"                        .write \\\r\n",
					"                        .format(\"delta\") \\\r\n",
					"                        .mode(\"overwrite\") \\\r\n",
					"                        .partitionBy(\"hash\") \\\r\n",
					"                        .save(BASE_ADLS_CONN_STR + STAGE_ORDER_GENIUS_DEFAULT_PATH.format(opco = OPCO))\r\n",
					"    logger.debug(\"organic default rankings: Write organic default rankings into STG container. OPCO = {0},path={1} Count = {2}\".format(OPCO,BASE_ADLS_CONN_STR + STAGE_ORDER_GENIUS_DEFAULT_PATH.format(opco = OPCO), df_ord_genius_stg.count()))\r\n",
					"    \r\n",
					"df_ord_genius_default=df_user_product_org_default.orderBy(\"ord_gen_user_id\",asc(\"rank\")).groupby(\"ord_gen_user_id\")\\\r\n",
					"                                                 .agg(collect_list(\"pod_id\").alias(\"ordergenius_default\"))\r\n",
					"                                                  \r\n",
					"df_ord_genius_default = df_ord_genius_default.join(df_removed_purc, df_ord_genius_default.ord_gen_user_id == df_removed_purc.user_id_del, \"left\")\\\r\n",
					"    .withColumn(\"ordergenius_default\",F.when(F.size(df_removed_purc.removed_purchases) > 0, F.array_except(df_ord_genius_default.ordergenius_default, df_removed_purc.removed_purchases)) \\\r\n",
					"    .otherwise(df_ord_genius_default.ordergenius_default)).drop(\"user_id_del\",\"removed_purchases\")\r\n",
					"\r\n",
					"df_ord_genius_default=df_ord_genius_default.withColumn(\"ordergenius_rank_partition\",udf_ord_rank_partition(col(\"ordergenius_default\"),lit(NO_OF_PROD_ORD_RANK),lit(NO_OF_PARTITONS_ORD_RANK)))\r\n",
					"                             \r\n",
					"logger.info(\" Calculate ordergeniusRankPartition and ordergeniusdefault values. OPCO = {0} Count = {1}\".format(OPCO, df_ord_genius_default.count()))"
				],
				"execution_count": 83
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**PAST PURCHASE MANAGEMENT**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import coalesce,array,create_map\r\n",
					"from pyspark.sql.functions import from_unixtime,unix_timestamp\r\n",
					"from pyspark.sql.functions import xxhash64\r\n",
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"logger.info(\"aggregate-products: Aggregate customer and their past-purchases. OPCO = {0}\".format(OPCO)) \r\n",
					"\r\n",
					"\r\n",
					"def apply_cdc(df_past_purch_delta_stg):\r\n",
					"    logger.info(\"\\t PAST PURCHASE:Past Purchase CDC. OPCO = {0}\".format(OPCO))\r\n",
					"    dt_past_purchase_mastr = DeltaTable.forPath(spark, BASE_ADLS_CONN_STR + TARGET_PAST_PURC_PATH.format(opco = OPCO))\r\n",
					"    dt_past_purchase_mastr.alias(\"mstr\") \\\r\n",
					"                    .merge(df_past_purch_delta_stg.alias(\"stg\"), \"mstr.user_id = stg.user_id\") \\\r\n",
					"                    .whenMatchedUpdate(set=\r\n",
					"                    {\r\n",
					"                        \"user_id\":\"stg.user_id\",\r\n",
					"                        \"hndl_id\":\"stg.hndl_id\",\r\n",
					"                        \"rtlr_card_id\":\"stg.rtlr_card_id\",\r\n",
					"                        \"mail_tx\":\"stg.mail_tx\",\r\n",
					"                        \"stat_cd\":\"stg.stat_cd\",\r\n",
					"                        \"opco_id\":\"stg.opco_id\",\r\n",
					"                        \"ordergenius_default\":\"stg.ordergenius_default\",\r\n",
					"                        \"ordergenius_rank_partition\":\"stg.ordergenius_rank_partition\",\r\n",
					"                        \"three_months_init\":\"stg.three_months_init\",\r\n",
					"                        \"six_months_init\":\"stg.six_months_init\",\r\n",
					"                        \"all_times_init\":\"stg.all_times_init\",\r\n",
					"                        \"last_order_init\":\"stg.last_order_init\",\r\n",
					"                        \"three_months\":\"stg.three_months\",\r\n",
					"                        \"six_months\":\"stg.six_months\",\r\n",
					"                        \"all_times\":\"stg.all_times\",\r\n",
					"                        \"last_order\":\"stg.last_order\",\r\n",
					"                        \"removed_purchases\":\"stg.removed_purchases\",\r\n",
					"                        \"id\":\"stg.id\",\r\n",
					"                        \"ordergenius_set_recency_decay\":\"stg.ordergenius_set_recency_decay\",\r\n",
					"                        \"first_lpurc_dt\":\"stg.first_lpurc_dt\", \r\n",
					"                        \"last_lpurc_dt\":\"stg.last_lpurc_dt\", \r\n",
					"                        \"last_purc_dates\":\"stg.last_purc_dates\", \r\n",
					"                        \"avg_qtys\":\"stg.avg_qtys\", \r\n",
					"                        \"qtys\":\"stg.qtys\", \r\n",
					"                        \"categories\":\"stg.categories\", \r\n",
					"                        \"category_averages\":\"stg.category_averages\", \r\n",
					"                        \"total_orders\":\"stg.total_orders\", \r\n",
					"                        \"total_amount\":\"stg.total_amount\", \r\n",
					"                        \"avg_ord_amt\":\"stg.avg_ord_amt\", \r\n",
					"                        \"max_tot_purc_qy\":\"stg.max_tot_purc_qy\", \r\n",
					"                        \"last_updated_at\":\"stg.last_updated_at\", \r\n",
					"                        \"last_updated_by\":\"stg.last_updated_by\", \r\n",
					"                        \"hash\": \"stg.hash\",\r\n",
					"                        \"hash_cd\":\"stg.hash_cd\",\r\n",
					"                        \"is_processed\":\"stg.is_processed\",\r\n",
					"                        \"is_active\":\"stg.is_active\"}) \\\r\n",
					"                    .whenNotMatchedInsertAll()\\\r\n",
					"                    .execute()\r\n",
					"    logger.info(\"Persist: Customers' past-purchase has been processed and persisted for OPCO = {0}, Count={1}, Path={2}\".format(OPCO, df_past_purch_delta_stg.count(), TARGET_PAST_PURC_PATH.format(opco = OPCO)))    \r\n",
					"\r\n",
					"\r\n",
					"def load_past_purchase(df_load,path):\r\n",
					"        df_load \\\r\n",
					"                .repartition(1, \"user_id\") \\\r\n",
					"                .sort(\"user_id\") \\\r\n",
					"                .write \\\r\n",
					"                .option(\"mergeSchema\", \"true\") \\\r\n",
					"                .format(\"delta\") \\\r\n",
					"                .mode(\"overwrite\") \\\r\n",
					"                .partitionBy(\"hash\") \\\r\n",
					"                .save(path)\r\n",
					"\r\n",
					"def past_purchase_delta_load(df_past_purchase_mstr,df_past_purc_stg):\r\n",
					"    \r\n",
					"    logger.info(\"PAST PURCHASE DELTA:Started getting New,Update and Deleted records for OPCO= {0}\".format(OPCO))\r\n",
					"\r\n",
					"    #NEW, UPDATE RECORDS  \r\n",
					"    df_past_purc_new_update=df_past_purc_stg.join(df_past_purchase_mstr,on=[\"user_id\",\"hash_cd\"],how=\"left_anti\")\r\n",
					"                       \r\n",
					"    #DELETE RECORDS\r\n",
					"    df_past_purchase_del=df_past_purchase_mstr.join(df_past_purc_stg,on=[\"user_id\"],how=\"left_anti\")\r\n",
					"    df_past_purchase_del=df_past_purchase_del.withColumn(\"is_processed\",lit(False))\\\r\n",
					"                                            .withColumn(\"is_active\",lit(False))              \r\n",
					"\r\n",
					"    df_past_purchase_total_delta=df_past_purc_new_update.select(*COLS_ORDER)\\\r\n",
					"                                    .union(df_past_purchase_del.select(*COLS_ORDER))\r\n",
					"    logger.info(\"PAST PURCHASE DELTA :Union of New,Update and Deleted records for loading into master OPCO = {0} ,New_Update_Count={1},Delete_Count={2},Union_Count={3}\".format(OPCO,df_past_purc_new_update.count(),df_past_purchase_del.count(),df_past_purchase_total_delta.count()))                                                                \r\n",
					"    return df_past_purchase_total_delta      \r\n",
					"\r\n",
					"\r\n",
					"# main starts here\r\n",
					"df_past_purc = df_users.join(df_usr_prd_all_times, df_users.user_id == df_usr_prd_all_times.user_id_all, \"inner\") \\\r\n",
					"                        .join(df_usr_prd_six_months, df_users.user_id == df_usr_prd_six_months.user_id_six_months, \"left\") \\\r\n",
					"                        .join(df_usr_prd_three_months, df_users.user_id == df_usr_prd_three_months.user_id_three_months, \"left\") \\\r\n",
					"                        .join(df_usr_prd_last_order, df_users.user_id == df_usr_prd_last_order.user_id_last_order, \"left\") \\\r\n",
					"                        .join(df_ord_genius_default,df_users.user_id==df_ord_genius_default.ord_gen_user_id,\"left\")\\\r\n",
					"                        .join(df_removed_purc, df_users.user_id == df_removed_purc.user_id_del, \"left\") \\\r\n",
					"                        .select(\"user_id\",\"hndl_id\", \"rtlr_card_id\", \"mail_tx\", \"stat_cd\", \"opco_id\", \\\r\n",
					"                        coalesce(\"ordergenius_default\",array()).alias(\"ordergenius_default\").cast(ArrayType(IntegerType())),\\\r\n",
					"                        coalesce(\"ordergenius_rank_partition\",create_map()).alias(\"ordergenius_rank_partition\").cast(MapType(StringType(),ArrayType(IntegerType()))),\\\r\n",
					"                        coalesce(\"three_months\",array()).alias(\"three_months_init\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"six_months\",array()).alias(\"six_months_init\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"all_times\",array()).alias(\"all_times_init\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"last_order\",array()).alias(\"last_order_init\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"three_months_purch_removed\",array()).alias(\"three_months\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"six_months_purch_removed\",array()).alias(\"six_months\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"all_times_purch_removed\",array()).alias(\"all_times\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"last_order_purch_removed\",array()).alias(\"last_order\").cast(ArrayType(IntegerType())), \\\r\n",
					"                        coalesce(\"removed_purchases\",array()).alias(\"removed_purchases\").cast(ArrayType(IntegerType())))\r\n",
					"\r\n",
					"\r\n",
					"df_past_purc = df_past_purc \\\r\n",
					"        .withColumn(\"id\", F.col(\"user_id\").cast(\"string\")) \\\r\n",
					"        .withColumn(\"ordergenius_set_recency_decay\",array([]).cast(ArrayType(IntegerType()))) \\\r\n",
					"        .withColumn(\"first_lpurc_dt\", F.lit(None).cast(StringType())) \\\r\n",
					"        .withColumn(\"last_lpurc_dt\", F.lit(None).cast(StringType())) \\\r\n",
					"        .withColumn(\"last_purc_dates\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"avg_qtys\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"qtys\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"categories\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"category_averages\",array([]).cast(ArrayType(StringType()))) \\\r\n",
					"        .withColumn(\"total_orders\",lit(0).cast(IntegerType())) \\\r\n",
					"        .withColumn(\"total_amount\",lit(0.00).cast(DecimalType(10,2))) \\\r\n",
					"        .withColumn(\"avg_ord_amt\",lit(0.00).cast(DecimalType(10,2))) \\\r\n",
					"        .withColumn(\"max_tot_purc_qy\",lit(0).cast(IntegerType())) \\\r\n",
					"        .withColumn(\"created_ts\", current_timestamp()) \\\r\n",
					"        .withColumn(\"created_by\",lit(USR_PAST_PURCHASE)) \\\r\n",
					"        .withColumn(\"last_updated_at\", from_unixtime(unix_timestamp(current_timestamp(), \"yyyy-MM-dd'T'HH:mm:ss'Z'\"),\"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\r\n",
					"        .withColumn(\"last_updated_by\",lit(USR_PAST_PURCHASE))\r\n",
					"                           \r\n",
					"hash_columns=[col for col in df_past_purc.schema.names if col not in EXCLUDE_COLUMNS]\r\n",
					"df_past_purc_stg =  df_past_purc.withColumn(\"hash\", f.abs(f.hash(f.col(\"user_id\"))%NO_OF_PARTITIONS))\\\r\n",
					"                                .withColumn(\"hash_cd\",xxhash64(*hash_columns))\\\r\n",
					"                                .withColumn(\"is_processed\",lit(False)) \\\r\n",
					"                                .withColumn(\"is_active\",lit(True)) \r\n",
					"\r\n",
					"logger.info(\"\\t Aggregating all datasets and created past-purchase document for OPCO = {0}\".format(OPCO))\r\n",
					"\r\n",
					"if(DeltaTable.isDeltaTable(spark, BASE_ADLS_CONN_STR + TARGET_PAST_PURC_PATH.format(opco = OPCO) )):\r\n",
					"    df_past_purchase_mstr=load_delta_toDF(BASE_ADLS_CONN_STR + TARGET_PAST_PURC_PATH .format(opco = OPCO))\r\n",
					"    df_past_purchase_mstr=df_past_purchase_mstr.filter(col(\"is_active\")==True)\r\n",
					"    logger.info(\"\\t PAST PURCHASE : Started CDC Update for Past Purchase\")\r\n",
					"    logger.info(\"\\t PAST PURCHASE : Past purchase users loaded from master. OPCO = {0}, Count={1}\".format(OPCO, df_past_purchase_mstr.count()))\r\n",
					"    if IS_DELTA_LOAD:              \r\n",
					"        if(df_past_purc_stg.count()>0):\r\n",
					"            apply_cdc(df_past_purc_stg)\r\n",
					"    else:\r\n",
					"        is_val_success= validate_count(df_past_purchase_mstr,df_past_purc_stg,VAL_CNT_THRESHOLD)        \r\n",
					"        if is_val_success :\r\n",
					"            df_past_purch_delta_stg=past_purchase_delta_load(df_past_purchase_mstr,df_past_purc_stg)\r\n",
					"            if(df_past_purch_delta_stg.count()>0):\r\n",
					"                apply_cdc(df_past_purch_delta_stg)\r\n",
					"        else:\r\n",
					"            logger.error(\"\\t Failed to override master since threshold is exceding \")\r\n",
					"            raise Exception(\"EXCEEDING_THRESHOLD\")                             \r\n",
					"else:\r\n",
					"    logger.info(\"\\t First Time Master Load\")\r\n",
					"    load_past_purchase(df_past_purc_stg,BASE_ADLS_CONN_STR + TARGET_PAST_PURC_PATH.format(opco = OPCO))\r\n",
					""
				],
				"execution_count": 85
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**LOAD PAST PURCHASE INTO COSMOS DATASTORE**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F \n",
					"\n",
					"logger.info(\"persist: Started persisting past-purchase in Cosmos for OPCO = {0}, Count={1}, Container={2}\".format(OPCO, df_past_purc.count(), COSMOS_CONTAINER_NM)) \n",
					"\n",
					"cfg = {\n",
					"    \"spark.synapse.linkedService\": COSMOS_LINKED_SERVICE_NM,\n",
					"    \"spark.cosmos.container\": COSMOS_CONTAINER_NM\n",
					"}\n",
					"\n",
					"df_past_purchase_mstr=load_delta_toDF(BASE_ADLS_CONN_STR + TARGET_PAST_PURC_PATH .format(opco = OPCO))\n",
					"df_past_purc_new_update= df_past_purchase_mstr.filter((col(\"is_processed\")==False))\n",
					"\n",
					"logger.info(\"PAST PURCHASE :Filtering data that are not processed to load into Cosmos ={0}\".format(df_past_purc_new_update.count()))\n",
					"\n",
					"if(df_past_purc_new_update.count()>0):\n",
					"    df_past_purc_data_store = df_past_purc_new_update \\\n",
					"        .withColumnRenamed(\"user_id\", \"userId\") \\\n",
					"        .withColumnRenamed(\"hndl_id\", \"hndlId\") \\\n",
					"        .withColumnRenamed(\"rtlr_card_id\", \"rtlrCardId\") \\\n",
					"        .withColumnRenamed(\"mail_tx\", \"email\") \\\n",
					"        .withColumnRenamed(\"stat_cd\", \"statCd\") \\\n",
					"        .withColumnRenamed(\"opco_id\", \"opco\") \\\n",
					"        .withColumnRenamed(\"three_months\", \"threeMonths\") \\\n",
					"        .withColumnRenamed(\"six_months\", \"sixMonths\") \\\n",
					"        .withColumnRenamed(\"all_times\", \"allTimes\") \\\n",
					"        .withColumnRenamed(\"last_order\", \"lastOrder\") \\\n",
					"        .withColumnRenamed(\"removed_purchases\", \"removedPurchases\") \\\n",
					"        .withColumnRenamed(\"ordergenius_rank_partition\", \"ordergeniusRankPartition\") \\\n",
					"        .withColumnRenamed(\"ordergenius_default\", \"ordergeniusDefault\") \\\n",
					"        .withColumnRenamed(\"ordergenius_set_recency_decay\", \"ordergeniusSetRecencyDecay\") \\\n",
					"        .withColumnRenamed(\"first_lpurc_dt\", \"firstLpurcDt\") \\\n",
					"        .withColumnRenamed(\"last_lpurc_dt\", \"lastLpurcDt\") \\\n",
					"        .withColumnRenamed(\"last_purc_dates\", \"lastPurcDates\") \\\n",
					"        .withColumnRenamed(\"avg_qtys\", \"avgQtys\") \\\n",
					"        .withColumnRenamed(\"total_orders\", \"totalOrders\") \\\n",
					"        .withColumnRenamed(\"total_amount\", \"totalAmount\") \\\n",
					"        .withColumnRenamed(\"avg_ord_amt\", \"avgOrdAmt\") \\\n",
					"        .withColumnRenamed(\"max_tot_purc_qy\", \"maxTotPurcQy\") \\\n",
					"        .withColumnRenamed(\"category_averages\", \"categoryAverages\") \\\n",
					"        .withColumnRenamed(\"last_updated_at\", \"lastUpdatedAt\") \\\n",
					"        .withColumnRenamed(\"last_updated_by\", \"lastUpdatedBy\") \\\n",
					"        .withColumnRenamed(\"is_active\",\"isActive\")\\\n",
					"        .drop(\"hash\", \"three_months_init\", \"six_months_init\", \"all_times_init\", \"last_order_init\",\"created_ts\",\"created_by\",\"hash_cd\",\"is_processed\")\n",
					"\n",
					"   \n",
					"    df_past_purc_data_store \\\n",
					"        .write \\\n",
					"        .format(\"cosmos.oltp\") \\\n",
					"        .options(**cfg) \\\n",
					"        .mode(\"append\") \\\n",
					"        .save()\n",
					"\n",
					"    logger.info(\"persist: Persisted past-purchase in Cosmos for OPCO = {0}, Count={1}, Container={2}\".format(OPCO, df_past_purc.count(), COSMOS_CONTAINER_NM))\n",
					"\n",
					"    dt_past_purchase_mastr = DeltaTable.forPath(spark, BASE_ADLS_CONN_STR + TARGET_PAST_PURC_PATH.format(opco = OPCO))\n",
					"    dt_delta = DeltaTable.forPath(spark, BASE_ADLS_CONN_STR + TARGET_PAST_PURC_PATH.format(opco = OPCO))\n",
					"    dt_delta.update(col(\"is_processed\")==False,{\"is_processed\": lit(True)})\n",
					"    logger.info(\"PAST PURCHASE :Updated Is_processed to True in the Delta table in ADLS = {0}\".format(OPCO))"
				],
				"execution_count": 87
			}
		]
	}
}